	
\section{Problem \& Motivation}
	Sparse linear algebra is a key component in various scientific simulations
	ranging from quantum physics to fluid and structural mechanics.
	Unfortunately many iterative numerical methods for sparse systems 
	and important building 	blocks of sparse linear algebra feature strong 
	data dependencies rendering them difficult to be parallelized.
	Typically	loop-carried dependencies may occur in
	iterative solvers  or preconditioners (e.g., Kaczmarz, Gauss-Seidel) 
	or write conflicts show up in the parallelization of building blocks such 
	as symmetric sparse matrix vector.
	Scalable, hardware-efficient parallelization of such kernels is known to be a 
	challenge. Multicoloring is a typical approach to enable parallelization
	of iterative solvers with distance-k dependency \eg the 
	red-black gauss-seidel solve distance-1 dependency problem.
	However most of the standard solutions suffer from low performance
	on modern hardware, are highly problem specific, or require tailored
    sparse matrix storage formats.

	\Acrshort{RACE} addresses these shortcomings by covering ideas from graph traversal
	and multicoloring to ensure data locality, appropriate levels of parallelism,
	and using hardware efficient parallelization schemes. The method is applicable
	to many problems (\ie matrix structures) and general sparse data storage formats.
	
%	In this paper we present a novel approach called \acrshort{RACE} that helps in
%	solving the general distance-k dependency problem for sparse kernels in a
%	hardware efficient manner. The \acrshort{RACE} method is motivated by the
%	shortcomings of multicoloring methods that are frequently used in this scenario.
%	The method works on simple data storage formats
%	like \acrfull{CRS}, and is highly scalable to meet the requirements of 
%	modern CPU based compute nodes. \medskip

\noindent\textbf{Outline}


\noindent	The paper is structured as follows. \Cref{sec:background} describes
	the underlying dependency problems and the conventional
	approaches used to solve such kind of dependencies. \Cref{sec:uniqueness} 
	shows the general issues that occur while using these conventional
	approaches. We then introduce the \acrshort{RACE} method in \cref{sec:RACE_method},
	it's uniqueness	and how its	basic design addresses the problems. 
	In \cref{sec:results} we compare \acrshort{RACE} performance to
	available standard approaches, and finally apply the \acrshort{RACE}
	method to parallelize an eigenvalue solver provided by \acrshort{MKL}.
	It demonstrates the necessity of the \acrshort{RACE} method to solve 
	large problems that is far beyond the reach of current state-of-the-art
	implementations.
	
	\begin{comment}
	In this paper we present a novel approach called \acrshort{RACE} that helps in
	solving the general distance-k dependency problem for sparse kernels in a
	hardware efficient manner. The \acrshort{RACE} method is motivated by the
	shortcomings of multicoloring methods that are frequently used in this scenario.
	The method uses a recursive level-based approach to find optimal permutations
	while preserving good data locality. A thorough performance analysis shows that
	our method achieves high hardware efficiency on modern multi-core architectures
	and it outperforms traditional \acrfull{MC} and \acrshort{MKL} implementations
	by a factor of 2--2.5$\times$. We are on par with \acrfull{ABMC} method for
	small matrices, while for large matrices we gain almost a factor of
	1.5--2$\times$. Owing to the success of parallel implementations of
	sparse kernels having dependencies we further demonstrate first results
	of parallel iterative FEAST eigen solver using CGMN internal solver.
	\end{comment}


\section{Background \& Related Work} \label{sec:background}
Many numerical methods that come across in sparse linear algebra are hard to 
be parallelized due to dependencies. 
%The two main class of dependencies that occur in this field are \DONE
%dependency as in Gauss-Seidel iteration or a \DTWO dependency as they 
%occur in \acrfull{KACZ} or \acrfull{SymmSpMV}. 

\begin{algorithm}[tb]
	\caption{\label{alg:symmSpMV} \acrshort{SymmSpMV} kernel,  $b=Ax$, in \acrshort{CRS} format.}
	\begin{algorithmic}[1]
		\For{$row=1:nrows$}
			\State{$diag\_idx=rowPtr[row]$}
			\State{$b[row] += A[diag\_idx]*x[row]$}
			\For{$idx=rowPtr[row]+1:rowPtr[row+1]$}
				\State{$b[row] += A[idx]*x[col[idx]]$}
				\State{$b[col[idx]] += A[idx]*x[row]$} 
			\EndFor
		\EndFor
	\end{algorithmic}
\end{algorithm}

 
As a typical example for \DTWO dependency problem we use \acrfull{SymmSpMV}. 
\Cref{alg:symmSpMV} shows the pseudo-code of \acrshort{SymmSpMV} kernel 
in \acrfull{CRS} \cite{CRS} matrix storage format. The kernel exploits the symmetry
of the matrix ($A_{ij} = A_{ji}$) to reduce storage size and overall 
memory traffic which is the biggest bottleneck on modern CPUs. 
However \acrshort{SymmSpMV} cannot be parallelized straightforward, two 
different threads working on two different rows in parallel could potentially 
write to the same element ($b[col[idx]]$) of the indirectly accessed $b$ vector
causing write conflicts.
In terms of graph theory this means a vertex (row in a matrix) and 
its \DTWO neighbor \cite{dist_k_def} cannot be operated in parallel.
In this paper we concentrate on such \DTWO dependency problems, although 
the underlying method and library is capable of handling 
general \DK dependencies.


A popular approach to solve such kind of dependency problem is  the
\acrfull{MC} techniques. Earliest work on coloring is the red-black 
Gauss-Seidel scheme~\cite{RBGS}, which was applied to  matrices with
 known sparsity pattern. 
Later \acrlong{MC} techniques have been expanded using graph theory
for general sparse matrices \cite{MC, COLPACK}.
Variants like \acrfull{ABMC} \cite{ABMC} have tried to improve the performance 
of the \acrshort{MC} methods. 
In \cite{feast_mc}, \acrshort{MC} has been applied to Kaczmarz iterative solver
having \DTWO dependency.
Specifically to \acrshort{SymmSpMV} there has been no previous attempt to use 
graph based techniques. General solutions for \acrshort{SymmSpMV} are 
locking-based methods and thread-private target arrays \cite{sparseX,thread_private_symm_spmv}.
Depending on the matrix structure these approaches could lead to performance
 degradation  due to serialization and massive increase in data traffic.
Recent researches in this direction use specialized storage formats like
 CSB \cite{CSB} or RSB \cite{RSB} but this requires rewriting of existing 
 code and lot of tuning. 

%to Kaczmarz iterative solver having  dependency. 
 
%Many solution to these dependency problems have been proposed, such as 
%locking method and thread-private target arrays \cite{sparseX,thread_private_symm_spmv}.
%But these approaches could lead to performance degradation depending
%on the matrix due to serialization or increase in data traffic respectively.
%Recent researches include in the direction of special storage formats like
% CSB \cite{CSB} or RSB \cite{RSB} but this require rewriting of existing 
% code and lot of tuning. Another popular approach in the field is matrix 
%reordering technique, on which we focus here.
%One of the earliest work on reordering is the red-black 
%Gauss--Seidel scheme~\cite{RBGS}. 
%Later it has been generalized for sparse matrices and \DK dependent 
%problems by well-known \acrfull{MC} approaches \cite{MC, COLPACK}. 
%Variants like \acrfull{ABMC} \cite{ABMC} have tried to
%improve the  performance of the \acrshort{MC} methods. In \cite{feast_mc},
%\acrshort{MC} was applied to \acrshort{KACZ} kernel having  \DTWO dependency. 


\section{Uniqueness of the approach} \label{sec:uniqueness}
Approaches like \acrshort{MC} can help in parallelization of kernels like \acrshort{SymmSpMV},
%\setlength{\belowcaptionskip}{-12pt}
\begin{figure}[tb]
	\centering
	\subfloat[\label{fig:mc_problem_a} Original ordering]{\scalebox{0.625}{\input{pics/alpha_problem/mc_alpha_unsymm_only_red_1.tex}}}\hspace{0.5em}
	\subfloat[After \acrshort{MC} applied]{\label{fig:mc_problem_b} \scalebox{0.625}{\input{pics/alpha_problem/mc_alpha_unsymm_only_red_2.tex}}}
	%	\includegraphics[scale=0.6]{pics/alpha_problem/mc_alpha_unsymm_only_red.tex}
	\caption{\label{fig:mc_problem} Illustration of data locality degradation due to \acrshort{MC}.
		Numbers represent thread id}
\end{figure}
however the method can destroy data locality of the matrix leading to low performance.
The reason for this is the nature of \acrlong{MC} 
permutation. For \DTWO coloring, \acrshort{MC}
groups rows that do not overlap in any column 
entries \cite{dist_k_def} (structurally orthogonal rows).
 These groups of rows are referred to as
a color. Due to this grouping one can parallelize all the rows within a color, 
as this ensures in our \acrshort{SymmSpMV} example (see \cref{alg:symmSpMV})
that threads operate on different rows having entirely different 
$col[idx]$ avoiding write conflicts in $b$ vector. \Cref{fig:mc_problem}
shows an illustration of the \acrshort{MC} permutation carried out on
a matrix (see \Cref{fig:mc_problem_a}) that initially has high data locality.
The matrix after multicoloring permutation is seen in \Cref{fig:mc_problem_b},
observe that within a color (for \eg red) none of the rows share same column entry. 
Traversing the elements of the original matrix row by row, would require only 
one time loading of the right-hand-side (RHS) vector entry, as the elements 



To demonstrate the effect we present in \cref{fig:motivation} performance and 
data transfer volumes of  \acrshort{SymmSpMV} operation on \texttt{Spin-26} matrix 
taken from quantum physics application. \Cref{fig:motivation_symm_spmv}
shows performance in gigaflops (\GF) as a function of thread (core) count 
on a single chip (1 socket) of \Intel \IVB (E5-2660) architecture 
clocked at 2.2\GHZ.
The general variant  of sparse matrix-vector multiplication (\acrshort{SpMV})
 using the full matrix  serves as the yardstick for comparison. 
Based on a naive performance analysis, as most of the sparse
kernels are highly memory bound on modern architectures
one would expect \acrshort{SymmSpMV} performance 
to be almost twice that of \acrshort{SpMV}, due to half the data traffic. 
However the \acrshort{SymmSpMV} implementation using \acrshort{MC} is more
than three times slower.


\begin{figure}[b]
	\subfloat[Scaling performance]{\label{fig:motivation_symm_spmv}\scalebox{0.8}{\input{pics/Spin-26/out/motivation_symm_spmv.tex}}}\hspace{1em}
	\subfloat[Data traffic]{\label{fig:motivation_data}\scalebox{0.8}{\input{pics/Spin-26/out/motivation_data_wo_RACE.tex}}}
	\caption{\label{fig:motivation}(a) Performance of \acrshort{SymmSpMV} with 
		\acrshort{MC} and \acrshort{ABMC} compared to \acrshort{SpMV}. 
		(b) Average main memory data traffic in bytes (B) per nonzero entry ($\acrshort{nnz}$) 
		of the full matrix as measured with \LIKWID tool \cite{LIKWID}.}
\end{figure}


\begin{itemize}
	\item Start with MC and ABMC weakness, Spin matrix
	\item Hardware efficiency and performance
	\item Stress on data locality
	\item Optimal permutations
	\item Work on simple data storage format like \acrshort{CRS}
\end{itemize}


\section{RACE method} \label{sec:RACE_method}
\begin{itemize}
	\item The three steps, stress on locality
	\item Recursion very brief and tree
\end{itemize}


\section{Results \& Contribution} \label{sec:results}
\begin{itemize}
	\item Test setup, brief is sufficient
	\item Start with Spin matrix, and little RLM
	\item SymmSpMV performance
\end{itemize}
\subsection{FEAST with RACE}
\begin{figure}[b]
	\centering
	\scalebox{0.56}{\input{pics/feast/skx/feast_BENCH3D}}
	%	\includegraphics[scale=0.6]{pics/alpha_problem/mc_alpha_unsymm_only_red.tex}
	\caption{Comparison of FEAST with default \acrshort{MKL} direct solver and 
		iterative solver CGMN parallelized using \acrshort{RACE}. The experiment is
		run on one socket of \SKX (Platinum 8160) chip.}
\end{figure}
\begin{itemize}
	\item Briefly describe FEAST
	\item Tell its implemented in MKL, we use Reverse Communication Interface (RCI) of
		\acrshort{MKL} to implement inner linear solver
	\item Tell why we need KACZ -> ill-conditioned linear system, robustness of KACZ -> only potential iterative solver. Tell 99\% of time it uses in this CGMN solver
	\item To our knowledge first implementation that uses iterative inner linear system, which is really inevitable to scale and for large size problems
	\item  Test setup, discrete laplacian, 10 inner eigenvalues, complex numbers 
	\item Comparison with default Pardiso
	\item Discuss on Big O ....
\end{itemize}

\subsection{Acknowledgments}
\begin{itemize}
	\item Thomas Gruber
	\item RRZE and RWTH for providing computations time.
\end{itemize}

\section{Future Work}

