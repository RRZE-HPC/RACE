Sparse linear algebra is a key component in many scientific simulations ranging from quantum physics to fluid and structural mechanics. However, iterative numerical methods and important building blocks of sparse linear algebra frequently feature strong data dependencies, making them difficult to parallelize. Typically, loop-carried dependencies occur in iterative solvers (e.g., Kaczmarz, Gauss-Seidel) or preconditioners and write conflicts show up in the parallelization of building blocks such as symmetric sparse matrix-vector multiplication. Scalable, hardware-efficient parallelization of such methods and kernels is known to be a challenge. Multi-coloring is a widely used approach to enable parallelization of iterative solvers with distance-k dependency. However, most of those standard solutions suffer from low performance on modern hardware, are highly problem specific, or require tailored sparse matrix storage formats.

RACE addresses these shortcomings by combining ideas from graph traversal and multi-coloring to ensure data locality, to generate appropriate levels of parallelism, and to enable hardware-efficient parallelization schemes. It is applicable to many problems (i.e., matrix structures) and general sparse data storage formats. A thorough performance analysis shows that RACE outperforms traditional multi-coloring methods and Intel MKL implementations. We are on par with algebraic block multi-coloring for small matrices, while for large matrices we gain almost a factor of 1.5–3×. The paper further shows how we use RACE to parallelize a sparse eigenvalue solver and demonstrate RACE's superiority in terms of performance and attainable problem sizes.
