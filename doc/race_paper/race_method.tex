% SIAM Shared Information Template
% This is information that is shared between the main document and any
% supplement. If no supplement is required, then this information can
% be included directly in the main document.
%Keeping in mind the observations from previous \cref{Sec:motivation}, one can observe that it would be best to maintain the non-zeros of matrix close to the diagonal. 
Improving data locality for sparse matrix computations like \SpMV  is closely related to bandwidth reduction of the matrix. This was recognized previously and has led to the pre-processing of matrix by applying bandwidth reduction algorithms like ``\RCMfull" (\RCM) \cite{RCM,RCM_Sparse_computation}. Thus we start  with a bandwidth reduction approach and then apply a coloring scheme which conserves data locality. If necessary this scheme is applied recursively to obtain sufficient level of parallelism. Within this recursive step we further avoid the problem of global synchronization between threads.

%Here, we aim to develop a method that does not distort this ideal permutations to a large extent and at the same time resolves \DK dependencies. 
 
Our method can be divided into three steps:
\begin{enumerate}
	\item Level construction
	\item Distance-k coloring
	\item Load balancing
\end{enumerate}

In the first step we basically apply bandwidth reduction algorithm including level construction and matrix reordering. We then use the information from the level construction step to form subsets of levels which allow for hardware efficient \DK coloring of the graph. Finally we present a concept to ensure load balancing between threads. As we apply these three steps recursively and work on the graph of the matrix we call this method  \RACE (\RACEfull).


%The method is strongly coupled to the hardware underneath and exploits only the parallelism as required by the hardware. If at the end of all these four steps one does not achieve sufficient parallelism, all the steps are recursively applied to selected sub-graphs of the matrix until sufficient parallelism is attained. Due to this recursive nature of our coloring method we cal it as ``\RACfullNoSpace" (\RAC).

To explain the method in an easier and illustrative way we choose a simple matrix which is associated with an artificially constructed two--dimensional stencil as shown in \cref{fig:2d-7pt-a}. The corresponding sparsity pattern and the graph of the matrix are shown in \cref{fig:2d-7pt-b} and \cref{fig:2d-7pt-c}, respectively.

\begin{figure}[tbhp]
	\centering
	\subfloat[\Stex]{\label{fig:2d-7pt-a}\includegraphics[width=0.17\textwidth , height=0.14\textheight]{pics/2d-7pt/stencil.pdf}}
	\hspace{0.8em}
	\subfloat[Sparsity
	 pattern]{\label{fig:2d-7pt-b}\includegraphics[width=0.38\textwidth , height=0.19\textheight]{pics/2d-7pt/2d_7pt_bw.eps}}
	\hspace{1em}
	\subfloat[Graph]{\label{fig:2d-7pt-c}\includegraphics[width=0.3\textwidth , height=0.18\textheight]{pics/2d-7pt/stencil_2d_7pt.pdf}}
	\caption{Basic structure of an artificially designed stencil (a) as well as corresponding sparsity pattern of its matrix discretization (b) and graph representation (c). A $8 \times 8$ lattice in two dimensions with Dirichlet boundary conditions are chosen. The stencil structure is designed for illustration purpose and does not represent any specific application scenario.}
	\label{fig:2d-7pt}
\end{figure}

\subsection*{Definitions}
The following basic definitions from graph theory are used in the following sections:
\begin{itemize}
	\item \textbf{Graph : } $G = (V,E)$ represents a graph where $V(G)$ belongs to set of vertices and $E(G)$ represents the edges in the graph. Note that we use $G$ for irreducible undirected graphs.
	\item \textbf{Neighborhood :} Neighborhood of vertex $u$ represented as $N(u)$ is defined as:
	\begin{equation*}
	  N(u) = \set{ v : uv \in E}.
	\end{equation*}
	\item \textbf{Subgraph :} In this paper a subgraph $H$ of graph $G$ specifically refers to the subgraph induced by vertices $V' \subseteq V(G)$ and is defined as
	\begin{equation*}
		H = (V', \set{ uv : uv \in E(G) \text{ and } u,v \in V'}).
	\end{equation*}
\end{itemize}

\subsection{Level Construction}\label{subsec:LEVEL_CONST}
The first step of \RACE is to determine different \textit{\levels} in the graph followed by a related permutation of the graph data structure. This is equivalent to well-known bandwidth reduction algorithms such as ``(Reverse) \CMfull" or ``Breadth First Search" (\BFS) \cite{BFS} based approaches. Though the RCM method is also implemented in \RACE, we use the  \BFS reordering in the following for better illustration purpose.
%\BFS can also be replaced with better bandwidth reduction algorithms like ``(Reverse) \CMfull".  
As a first step we choose a root vertex which is assigned to the first \level ($L(0)$). All other \levels ($L(i)$ for $i > 0$) are defined to  contain vertices that are in neighborhood of vertices in previous \level $L(i-1)$ but not in \level $L(i-2)$ \cite{BFS_level_def} \ie
\begin{equation}\label{eq:level}
L(i) = 
\begin{cases}
	  u : u \in N(L(i-1)) \cap \overline{N(L(i-2))}  & \text{ if } i \neq 0, \\
	 root & otherwise.
\end{cases}   
\end{equation}

From \cref{eq:level} one finds that the $i$-th \level consist of all vertices that have a minimum distance of $i$ from the root node. See \Cref{alg:BFS} on how to determine minimum distance of each node from the root. The total number of \levels obtained with this graph traversal is denoted as \totalLvl. \Cref{fig:2d_7pt_level_construction} shows the \totalLvl=14 \levels of our artificial stencil discretization, where the index of each vertex ($v$) refers to the vertex number and the superscript represents the \level number, \ie
\begin{equation}\label{eq:node_notation}
	v^i \implies v \in L(i).
\end{equation}
Note that this is substantially different to the \levels used in ``level-scheduling" \cite{saad} approach which applies a ``Depth First Search".

\setlength{\fboxsep}{0pt}%

\begin{figure}[tbhp]
	\centering
	\subfloat[Level construction]{\label{fig:2d_7pt_level_construction}\includegraphics[height=0.18\textheight,width=0.32\textwidth]{pics/level_construction/stencil_2d_7pt}
			\begin{picture}(0,0)
			\put(-44,68){\fbox{\includegraphics[height=1.4cm]{pics/level_construction/FDM_2d_7pt_non_perm}}}
			\end{picture}
		}
	\hspace{1em}
	\subfloat[Permuted graph ($G'$)]{\label{fig:2d_7pt_perm}\includegraphics[height=0.18\textheight,width=0.32\textwidth]{pics/permutation/stencil_2d_7pt}
			\begin{picture}(0,0)
			\put(-43.5,68){\fbox{\includegraphics[height=1.4cm]{pics/permutation/FDM_2d_7pt_perm}}}
			\end{picture}
		}
	\hspace{1em}
	\subfloat[]{\label{fig:2d_7pt_levelPtr}\includegraphics[height=0.18\textheight,width=0.07\textwidth]{pics/permutation/levelPtr}}
	\caption{Levels of the original graph (a) and the permuted graph (b) for the \stex. Insets show associated sparsity pattern of the stencils. The entries of  the \levelPtr associated with $G'$ are presented in (c).}
	\label{fig:2d-7pt_step_1_2}
\end{figure}


%\subsection{Permutation}\label{subsec:PERM}
After the \levels have been determined, the matrix is permuted in the order of its \levels, such that the vertices in $L(i)$ are stored consecutively and appear before that of $L(i+1)$. \Cref{fig:2d-7pt_step_1_2} displays the graph ($G' = P(G)$) of the \stex after applying this permutation ($P$) and demonstrates the enhanced spatial locality of the vertices within and between \levels (see \cref{fig:2d_7pt_perm}) as compared to the original (lexicographic) numbering (see \cref{fig:2d_7pt_level_construction}).  Until now the procedure is the same as \BFS (or \RCM). 

As \RACE uses information about the \levels for resolving dependencies in the coloring step, we store the entry point to each \level in the permuted data structure (of $G'$) in an array $\levelPtr[0:$ \totalLvl$]$, \ie \levels on $G'$ can be identified as:
\begin{equation*}
	L(i) = \set{ u : u \in [\levelPtr[i]:(\levelPtr[i+1]-1)] \text{ and } u \in V(G')} .
\end{equation*}

The entries of \levelPtr for \stex are shown in \cref{fig:2d_7pt_levelPtr}. 
%, and one could easily read from \levelPtr that vertices from $\levelPtr(4)=7$ to $\levelPtr(5)-1=10$ belongs to $L(4)$.
 
 \subsection{Distance-k coloring} \label{subsec:DK}
 The \levels generated above serve as the base for our \DK coloring procedure as they contain information about the neighborhood relation between the vertices of any two \levels. Following the definition in~\cite{dist_k_def}, two vertices are called \DK neighbors if the shortest path connecting them consists of at most $k$ edges.
%In this section we introduce the idea of \DK neighbor and show how this idea can be used to color the matrix with the help of the level information that we already have in hand.
This implies $u$ is a \DK neighbor of $v$ (denoted as $u\xrightarrow{k}v$)  if
 \begin{equation}\label{eq:dk}
	  u\xrightarrow{k}v  \iff  v \in \set{ u \cup N(u) \cup N^2(u) \cup ... N^k(u) }.	 
 \end{equation}
 For undirected graphs as considered in this work  $u\xrightarrow{k}v$ also implies $v\xrightarrow{k}u$. Based on this definition we consider two vertices to be \DK independent if they are not \DK neighbors. Thus,  \levels $L(i)$ and $L(i\pm(k+j))$  of the permuted graph $G'$ are \DK independent for all $j\geq1$ as shown in the following:
  \begin{corollary}\label{corollary_dk}
   $L(i)$ and $L(i\pm(k+j))$ are \DK independent $\forall j\geq1$. 
  \end{corollary}
  \begin{proof}
  	We prove by contradiction. Let there exist $u,v \in V(G')$ such that  $u \in L(i)$ and $v \in  L(i \pm (k+j)) \forall j\geq1$. Assume $u,v$ are \DK neighbors ($u\xrightarrow{k}v$). From \cref{eq:level}, \cref{eq:dk} and the fact $G'$ is undirected we get 
  	\begin{align*}
	  	u\xrightarrow{k}v \iff & v \in \set{L(i) \cup L(i \pm 1) \cup ... \cup L(i \pm k)} \\
	  	\implies & v \notin L(i \pm (k+j)) \text{  } \forall j \geq 1
  	\end{align*}
  	which contradicts to $v \in L(i \pm (k+j) ) \forall j \geq 1$. This implies $u$ and $v$ are \DK independent.
  \end{proof}

\Cref{corollary_dk} implies that if there is a gap of \emph{\atleast} one \level between any two \levels ($L(i), L(i+2)$ for example) all the vertices between them are \DONE independent. Similarly if the gap consists of \emph{\atleast} two \levels between any two \levels ($L(i), L(i+3)$ for example) we have \DTWO independent \levels.
  
 \begin{figure}[tbhp]
 	\centering
 	\subfloat[\DONE independent \levelGroups]{\label{fig:2d_7pt_d1}\includegraphics[height=0.2\textheight,width=0.4\textwidth]{pics/dk_coloring/stencil_2d_7pt_d1}}
 	\hspace{2.5em}
 	\subfloat[\DTWO independent \levelGroups]{\label{fig:2d_7pt_d2}\includegraphics[height=0.23\textheight,width=0.48\textwidth]{pics/dk_coloring/stencil_2d_7pt_d2_with_lg}}
 	\caption{Forming \DONE and \DTWO independent \levelGroups for the \stex.}
 	\label{fig:2d-7pt_d1_d2}
 \end{figure}
 
 % {\CA Maybe more explanation is required.} 
 The weak definition used in \cref{corollary_dk} offers many choices for forming \DK independent sets of vertices, which can then be executed in parallel. 
 In  \cref{fig:2d-7pt_d1_d2} we present one example each for \DONE (\cref{fig:2d_7pt_d1}) and \DTWO (\cref{fig:2d_7pt_d2}) coloring of our \stex. The \DONE coloring uses a straight forward approach by assigning two colors to alternating \levels, i.e. \levels of a color can be calculated concurrently. In case of \DTWO independency we refrain from using three colors but instead aggregate two adjacent \levels to form a \textit{\levelGroup} (denoted by $T(i)$) and perform a \DONE coloring on top of those groups. This guarantees that vertices of two \levelGroups of the same color are \DTWO independent and can be executed in parallel. Here, the vertices in $T(0)$, $T(2)$, $T(4)$, $T(6)$ can be operated by four different threads in parallel, i.e. one thread per \levelGroup.  After synchronisation the remaining four blue \levelGroups can also be executed in parallel. Note, that within a single \levelGroup all verticies are computed in their original order enabling high spatial locality for accessing them. This idea can be generalized such that for \DK coloring, each \levelGroup contains \atleast $k$ adjacent \levels but the number of \levels per \levelGroup may be different. Thus formed \levelGroups are then \DONE colored. Then, all \levelGroups within a color can be executed in parallel. This simple approach allows to generate workload for $\frac{\totalLvlMATH}{2 k}$ threads if \DK coloring is requested.
 
However, choosing the same number of \levels for each \levelGroup may lead to severe load imbalances depending on the matrix structure. In particular the use of bandwidth reduction schemes such as BFS or RCM  will further worsen that problem due to the lens like shape of the reordered matrix (see \cref{fig:2d_7pt_perm} inset) leading to low workload for matrix rows at the top and bottom of the matrix. Compare \eg $T(0)$ and $T(7)$ with  $T(3)$ and $T(4)$ in \cref{fig:2d_7pt_d2}.
 %coloring by aggregating consecutive levels into \levelGroups (denoted by $T(i)$). In  \Cref{fig:2d_7pt_d1}  and \Cref{fig:2d_7pt_d2} we present one example for \DONE and \DTWO coloring of our \stex, respectively. The \DONE coloring uses straight forward approach by assigning two colors to alternating levels, i.e. \levelGroup and \level is equivalent here. For and applying a \DONE coloring on top of those groups as shown for \DTWO coloring in \cref{fig:2d_7pt_d2}.  In this context \Cref{fig:2d-7pt_d1_d2} contains two potential colorings for \DONE independent \levels as \DTWO coloring also solves \DONE dependencies.  One could also group some more of nearby \levels together to form a \levelGroup, and make this \DONE or \DTWO independent of other \levelGroups. The $i$-th \levelGroup would be denoted by $T(i)$. Difference between \level and  \levelGroup can be seen in \cref{fig:2d_7pt_d2}, for \cref{fig:2d_7pt_d1} \levelGroup and \level coincides. In principle one could compute on all independent \levelGroups in parallel, but sequentially within a \levelGroup, \ie for example in \cref{fig:2d_7pt_d2} $T(0)$, $T(2)$, $T(4)$, $T(6)$ can be operated by four different threads in parallel and in the next sweep rest \levelGroups. For the configurations seen in \cref{fig:2d-7pt_d1_d2} then we have $\frac{\totalLvlMATH}{2}$ and $\frac{\totalLvlMATH}{4}$ parallelism for \DONE and \DTWO kernels respectively.
 %However the problem with the configurations like the one seen in \cref{fig:2d-7pt_d1_d2} {\CA is that there, check Holger's comment} is load imbalances between threads because the number of rows (\nrows) per \levelGroup is not distributed evenly. As seen here in the case of \stex the threads working on extreme ends of graph (\eg $T(1), T(7)$) have a small amount of work compared to the threads working on middle (\eg $T(3), T(4)$). 
  
  \subsection{Load balancing}\label{subsec:LB} 
The RACE load balancing scheme tries to balance the workload between level groups within each color for a given number of threads while maintaining data locality and \DK constraint between the two colors. Level groups containing low workload will grab adjacent levels from neighboring level groups and overloaded level groups will shift levels to adjacent level groups. As a target for the load balancing scheme one can balance the number of rows (i.e. vertices) of the \levelGroups ($\nrowsMath(T(i))$) or the number of non-zeros (i.e. edges) of the \levelGroups ($\nnzMath(T(i))$). Both variants are supported and we choose balancing by number of rows in the following to demonstrate our load balancing algorithm, which can be found in \cref{alg:LB}.
%Depending on the matrix each \levelGroup contains different number of rows, that leads to load imbalances as seen above in \cref{subsec:DK}. \Inorder to avoid this problem we employ a load balancing scheme. At this step  we plug in details from the hardware side  namely the total parallelism required by the hardware. The idea is to exploit only the required parallelism while at the same time maintain \DK constraint seen in \cref{corollary_dk}. To balance the load more nearby \levels would be added to a \levelGroup ($T(x)$) which has less number of rows ($\nrowsMath(T(x))$) and at \levelGroup where we have considerably big \levels only sufficient amount of \levels to maintain \DK constraint would be assigned. Assigning nearby levels instead of others further helps in preserving data locality. 
 
Basically our algorithm tries to reduce the variances of the workload in the level groups, i.e. the number of rows ($\nrowsMath(T(i))$) in each \levelGroup $T(i)$. For a given set of \levelGroups we calculate mean and variance of $\nrowsMath(T(i))$ within each color, i.e. red and blue color.  
 %For example in \cref{fig:2d_7pt_d2} we need to calculate mean of $T\_size$ of all \levelGroups in red sweep and blue sweep separately. 
The overall variance, which is target of our minimization procedure, is then found by summing up the variances between colors. \Inorder to reduce this value we first select the two \levelGroups with largest negative/positive deviation from mean and try to add/remove levels to/from these \levelGroups (see top row of \cref{fig:lb_alg}). When removing \levels from a \levelGroup one has to take care that the \DK coloring is not violated by keeping at least $k$ levels in a \levelGroup. The shift of \levels is done via a pointer array denoted as $T\_ptr$, which points to the beginning of each \levelGroup (see \cref{fig:2d_7pt_lb}), avoiding any copy operation. If shifting levels between the two level groups with largest deviation does not lead to a lower overall variance no levels are exchanged and we choose the next pair of level groups according to a ranking which is based on the absolute deviation from the mean (see \cref{alg:LB} for details on implementation) and continue. Doing this process in an iterative way we finally end up in a state with lowest overall variance at which no further moves are possible due to violation of \DK dependency or increase in overall variance. \Cref{fig:lb_alg} shows the load balancing procedure under \DTWO constraint for some initial mapping of 17 levels to six level group. Applying the procedure to our  \stex of size $16 \times 16$  requesting \DTWO coloring and ten level groups leads to the mapping shown in \cref{fig:2d_7pt_lb}. 
%One could also do this entire load balancing based on number of non-zeros (\nnz) rather than \nrows, in this case $T\_size(i)=\nnzMath(T(i))$ (non-zeros in $T(i)$).
  
   \begin{figure}[tbhp]
   	\centering
   	\includegraphics[height=0.22\textheight,width=\textwidth]{pics/load_balancing/lb_alg/lb_all}
   	\caption{All steps of the load balancing scheme applied to an arbitrarily chosen initial distribution of 17 levels into six level groups for \DTWO coloring. Rebalancing steps are performed clockwise starting from top-left. $mean\_r$ and $mean\_b$ denote the current average number of rows per \levelGroup and color. $var$ is the overall  variance}
   	\label{fig:lb_alg}
   \end{figure}
   
    
    \begin{figure}
      \begin{minipage}[c]{0.63\textwidth}
      	\includegraphics[height=0.26\textheight,width=0.9\textwidth]{pics/load_balancing/2d-7pt/stencil_2d_7pt}
      \end{minipage}\hfill
      \begin{minipage}[c]{0.34\textwidth}
      	\caption{After load balancing for five threads and \DTWO dependency on \stex, domain size $16 \times 16$. Note that \levelGroups at extreme end have more \levels due to less \nrows in each \level, while \levelGroups in middle having bigger \levels maintain two levels to preserve \DTWO constraint.
      	} \label{fig:2d_7pt_lb}
      \end{minipage}
     \end{figure}
     

	\subsection{Recursion}\label{subsec:REC}
As discussed in \cref{subsec:DK} the maximum degree of parallelism is limited by the total number of levels (\totalLvl) and may be further reduced by level aggregation in the load balancing step.  To match the high levels of parallelism in modern compute devices we search for further parallelism within the \levelGroups.  Contrary to methods like \MCfull we didn't require all vertices in a \levelGroup to be \DONE (or \DK in general) independent of each other, rather we had a weak constraint as prescribed by \cref{corollary_dk}. There can exist more parallelism within the \levelGroups which we can interpret as \subgraphs.  Thus, we apply the three steps of our method recursively on selected \subgraphs to exploit the parallelism within them.  
%i.e.  \levelGroups which are recursively refined will be computed by multiple threads whereas unmodified \levelGroups will be assigned to a single thread. 

In the following we first demonstrate the basic idea in the context of \DONE dependencies where all dependencies can be resolved within the given \levelGroup by design. However, for $k>1$ vertices in a \levelGroup may have a \DK dependencies through vertices in adjacent \levelGroups. We generalize our procedure to \DK dependencies as a second step.

In order to visualize the basic concepts easily and discuss important corner cases of the recursive approach we choose the simple graph shown in \cref{fig:rec_d1_s1_a}, which is not correlated to our \stex. To distinguish between \levelGroups at different stages $s$ of our recursive procedure we add a subscript to the levels ($L_s(i)$) and \levelGroups ($T_s(i)$) indicating the stage of recursion at which they are generated, with $s=0$ being the original distribution before recursion is applied to any \subgraph. Finally, we apply the recursive scheme to our \stex and introduce proper sub-graph selection as well as global load balancing strategies.

%also for most of the graphs as we approach the limit of parallelism there is not much room for load balancing, leading to imbalances. Depending on matrix and hardware underneath this might lead to inefficient utilization of resources. \Inorder to avoid this problem we use the concept of recursion and exploit further parallelism if required by the hardware. Idea here is to intelligently select sub-graph(s) of the entire matrix and apply all the three steps recursively on this \subgraph. In the following we will show this concept in the context of \DONE and  later we will extent it to \DK dependencies. In order to explain the basic concepts easily and include all the corner cases we demonstrate the procedure on a simple graph which is shown in \cref{fig:rec_d1_s1_a}, later we will show the results of applying recursion on \stex. Further we will discuss on the method employed to select proper sub-graph and to have a globally balanced load.
	
	\subsubsection{Distance-$1$ dependency}
%\LevelGroups which we constructed till now belong to first stage of recursion ($s=0$). Stage number of recursion is denoted using subscript, \ie for example $L_s(i)$ denotes \level $i$ of stage $s$. Contrary to methods like \MCfull we didn't require each nodes in a color to be \DONE independent of each other, rather we had a weak constraint as prescribed by \cref{corollary_dk}. Due to this there can exist more parallelism within a \levelGroup. 

For the \DONE coloring of the graph in \cref{fig:rec_d1_s1} we find that the third \levelGroup of the initial stage (T$_0$(2)) still contains vertices which are \DONE independent, e.g. vertices $3 \not\xrightarrow{1} 4$ ($3$  \DONE independent to $4$), $3 \not\xrightarrow{1} 5$, $3 \not\xrightarrow{1} 6$ and $4 \not\xrightarrow{1} 6$, implying each of these pairs can be computed in parallel without any \DONE conflicts. This parallelism has not been exploited in first stage ($s=0$)  as vertices in $L_0(i)$ (here $i=2$) are chosen such that they are \DONE neighbors of \level $L_0(i-1)$ ignoring any vertex relations in $L_0(i)$. 
%We now apply our method recursively to the levels or \levelGroups to identify further paralleism within them.  	
     \begin{figure}[thbp]
     	\centering
     	\subfloat[Example graph]{\label{fig:rec_d1_s1_a}\includegraphics[width=0.26\textwidth, height=0.14\textheight]{pics/recursion/d1/rec_graph_s1/recursion_graph_1}}
     	\hspace{1.5em}
     	\subfloat[Stage 1 levels in graph]{\label{fig:rec_d1_s1_b}\includegraphics[width=0.26\textwidth, height=0.14\textheight]{pics/recursion/d1/rec_graph_s1/recursion_graph_2}}
     	\hspace{1.5em}
     	\subfloat[\DONE coloring]{\label{fig:rec_d1_s1_c}\includegraphics[width=0.26\textwidth, height=0.14\textheight]{pics/recursion/d1/rec_graph_s1/recursion_graph_3}}
        \caption{Shows potential for more parallelism. $T_0(1),T_0(2)$ and $T_0(3)$ has more parallelism. Note that the graph shown here is not related to the previous \stex.}
     	\label{fig:rec_d1_s1}
     \end{figure}

Recursion starts with the selection of a \subgraph of the matrix, which is discussed in more detail later (see \cref{subsec:subgraph_selection}). Here we choose \subgraph induced by $T_0(2)$. The  \subgraph can be isolated from rest of the graph since  \DONE coloring step in stage 0 has already made \levelGroups independent of each other. Now we just need to repeat the three steps explained previously (\cref{subsec:LEVEL_CONST} - \cref{subsec:LB}) on this \subgraph.
%to exploit parallelism within this \subgraph.
   
     \begin{figure}[thbp]
     	\centering
     	\subfloat[]{\label{fig:rec_d1_s2_a}\includegraphics[width=0.26\textwidth, height=0.14\textheight]{pics/recursion/d1/rec_graph_s2/recursion_graph_stage2_1}}
     	\hspace{2.25em}
     	\subfloat[]{\label{fig:rec_d1_s2_b}\includegraphics[width=0.08\textwidth, height=0.14\textheight]{pics/recursion/d1/rec_graph_s2/recursion_graph_stage2_2}}
     	\hspace{1.75em}
     	\subfloat[]{\label{fig:rec_d1_s2_c}\includegraphics[width=0.07\textwidth, height=0.14\textheight]{pics/recursion/d1/rec_graph_s2/recursion_graph_stage2_3}}
     	\hspace{1.75em}
     	\subfloat[]{\label{fig:rec_d1_s2_d}\includegraphics[width=0.07\textwidth, height=0.14\textheight]{pics/recursion/d1/rec_graph_s2/recursion_graph_stage2_4}}
	     \hspace{1.75em}
	     \subfloat[]{\label{fig:rec_d1_s2_e}\includegraphics[width=0.07\textwidth, height=0.14\textheight]{pics/recursion/d1/rec_graph_s2/recursion_graph_stage2_5}}
     	\caption{Applying recursion to \subgraph induced by $T_0(2)$. \Cref{fig:rec_d1_s2_b} shows the isolated \subgraph, \cref{fig:rec_d1_s2_c} presents the level construction step on the \subgraph. Two potential \DONE colorings of this  \subgraph are shown in \cref{fig:rec_d1_s2_d,fig:rec_d1_s2_e}.}
     	
     	\label{fig:rec_d1_s2}
     \end{figure}
     
     \Cref{fig:rec_d1_s2} shows an illustration of applying the first recursive step ($s=1$) on $T_0(2)$, where we extent the definition of the vertex numbering in \cref{eq:node_notation} to the following:
	 \begin{equation}
	    v^{i,j,k...} \implies v \in \set{L_0(i) \cap L_1(j) \cap L_2(k) \cap ...} 
	 \end{equation}
	 
     In this case at the end of recursion (\cf \cref{fig:rec_d1_s2_d}, \cref{fig:rec_d1_s2_e}) on $T_0(2)$ we obtain parallelism for two more threads. Note that the \subgraphs might have islands (group of vertices that are not connected to rest of the graph), \eg vertex 3 and vertices 4,5,6 form two islands in \cref{fig:rec_d1_s2_b}. Since an island is totally disconnected from the rest of the graph it can be executed in parallel to rest of the graph. To take advantage of this the starting node in next island is assigned with an increment of two levels, as seen in \cref{fig:rec_d1_s2_c}. Therefore multiple valid \DONE configurations (\cf \cref{fig:rec_d1_s2_d,fig:rec_d1_s2_e}) exist and the selection of the optimal one will be done in the final load balancing step as described in \cref{subsec:LB}.    
     
     With this recursive process we were able to find independent \levelGroups ($T_{s+1}$) within \levelGroup of previous stage ($T_s$) and therefore the thread which works on $T_s$ has to spawn threads to parallelize within $T_{s+1}$.
     
	\subsubsection{Distance-$k$ dependencies with $k>1$}

In general, however, it is not sufficient in the recursion step to consider only the \subgraphs induced by \levelGroups as can be seen in \cref{fig:rec_d2_wrong_a} for \DTWO coloring. Applying  the three steps (see \cref{fig:rec_d2_wrong_b}, \cref{fig:rec_d2_wrong_c} and \cref{fig:rec_d2_wrong_d})  to  the \subgraph induced by $T_0(1)$ does not guarantee \DTWO independency between the new \levelGroups $T_1(0)$ and $T_1(2)$. It is obvious that for general \DK colorings two vertices $a,b$ within a \levelGroup might be connected by a shared vertex $c$ outside the \levelGroup. 
%as they
%For \DK the same procedure as \DONE applies, except with a slight difference in selecting the \subgraph. In \DONE we considered \subgraphs induced by \levelGroups, but for \DK coloring this is not sufficient. As seen in \cref{fig:rec_d2_wrong_a} for \DTWO coloring the selection of $T_0(1)$ as \subgraph and applying the three steps (see \cref{fig:rec_d2_wrong_b}, \cref{fig:rec_d2_wrong_c} and \cref{fig:rec_d2_wrong_d}) did not guarantee \DTWO independency within \levelGroup $T_1$ of the \subgraph, for \eg $T_1(0)$ and $T_1(2)$ are not \DTWO independent (\cf \cref{fig:rec_d2_wrong_d}). This is due to the fact that for $k>1$ dependencies two vertices $a,b$ within a \subgraph might be connected to a common vertex ($c$) outside the \subgraph leading to a \DK dependency between $a$ and $b$. In \cref{fig:rec_d2_wrong} we see 	$3 \xrightarrow{1} 1 \text{ \& } 6 \xrightarrow{1} 1 	\implies 3 \xrightarrow{2} 6$, but since vertex $1$ was not in the \subgraph considered we missed this dependency. 
     \begin{figure}[thbp]
     	\centering
     	\subfloat[]{\label{fig:rec_d2_wrong_a}\includegraphics[width=0.23\textwidth, height=0.13\textheight]{pics/recursion/d2/wrong/recursion_graph_wrong_1}}
     	\hspace{0.6em}
     	\subfloat[]{\label{fig:rec_d2_wrong_b}\includegraphics[width=0.23\textwidth, height=0.07\textheight]{pics/recursion/d2/wrong/recursion_graph_wrong_2}}
     	\hspace{0.6em}
     	\subfloat[]{\label{fig:rec_d2_wrong_c}\includegraphics[width=0.23\textwidth, height=0.07\textheight]{pics/recursion/d2/wrong/recursion_graph_wrong_3}}
     	\hspace{0.6em}
     	\subfloat[]{\label{fig:rec_d2_wrong_d}\includegraphics[width=0.23\textwidth, height=0.13\textheight]{pics/recursion/d2/wrong/recursion_graph_wrong_4}}
     	\caption{Two \levelGroups generated by a \DTWO coloring (\cref{fig:rec_d2_wrong_a}). \Cref{fig:rec_d2_wrong_b} shows the \subgraph induced by \levelGroup $T_0(1)$. Level construction on the selected \subgraph is shown in \cref{fig:rec_d2_wrong_c}. Forming \DTWO independent \levelGroups on these levels does not guarantee a \DTWO independency between the newly generated \levelGroups of same sweep (color) as seen in \cref{fig:rec_d2_wrong_d}}
     	\label{fig:rec_d2_wrong}
     \end{figure}
Thus, our three step procedure must be applied to a \subgraph which contains the actual \levelGroup ($T_s(j)$) as well as its \DKM neighborhood which is defined as 
\begin{equation*}
	   N^{k-1}(T_s(j)) = \set{u : u \xrightarrow{k-1} v \text{  } \forall v \in T_s(j) \text { and } u \notin T_s(j)}.
\end{equation*}
This ensures that there is no vertex outside the \subgraph which can mediate a \DK dependency between vertices in the embedded \levelGroup ($T_s(j)$). We can now construct the new levels on this \subgraph considering the neighborhood but only store the vertices in the new \levels $L_{s+1}(:)$ which are in the embedded \levelGroup ($T_s(j)$). Next we apply \DK coloring by aggregation of the new levels leading to a set of \levelGroups $T_{s+1}(:)$ within $T_s(j)$. \Cref{fig:rec_d2_correct} uses this approach to resolve the conflict demonstrated in \cref{fig:rec_d2_wrong_d}. \Cref{fig:rec_d2_correct_b} presents the \subgraph containing the selected \levelGroup $T_0(1)$ and its \DONE neighborhood. %For \DTWO this would mean we have to include $1^{st}$ interface level, the new selection is illustrated in \cref{fig:rec_d2_correct} (note the region shaded in blue). If we then do all the three steps with the newly created \subgraph, the final result will preserve \DTWO coloring (see \cref{fig:rec_d2_correct_c}). In the example it can be observed vertices $3$ and $6$ which previously had same color now get assigned to different colors (see \cref{fig:rec_d2_correct_d}). Note that the interface levels have to be considered only in the first step namely level construction in the rest of the steps we just need to consider target \subgraphs induced by the \levelGroups \ie in \cref{fig:rec_d2_correct} the \subgraph induced by $T_0(1)$.   
\begin{figure}[thbp]
     	\centering
     	\subfloat[]{\label{fig:rec_d2_correct_a}\includegraphics[width=0.22\textwidth, height=0.13\textheight]{pics/recursion/d2/correct/recursion_graph_correct_1}}
     	\hspace{0.6em}
     	\subfloat[]{\label{fig:rec_d2_correct_b}\includegraphics[width=0.22\textwidth, height=0.105\textheight]{pics/recursion/d2/correct/recursion_graph_correct_2}}
     	\hspace{0.6em}
     	\subfloat[]{\label{fig:rec_d2_correct_c}\includegraphics[width=0.22\textwidth, height=0.105\textheight]{pics/recursion/d2/correct/recursion_graph_correct_3}}
     	\hspace{0.6em}
     	\subfloat[]{\label{fig:rec_d2_correct_d}\includegraphics[width=0.22\textwidth, height=0.105\textheight]{pics/recursion/d2/correct/recursion_graph_correct_4}}
     	\hspace{0.6em}
     	\caption{Correct procedure for \DTWO coloring of \levelGroup $T_0(1)$. The \subgraph as shown in \cref{fig:rec_d2_correct_b} contains \levelGroup $T_0(1)$ and its \DONE neighborhood. Level construction step is applied to this \subgraph in \cref{fig:rec_d2_correct_c}. \DTWO coloring by level aggregation leading to \levelGroups of stage 2 is shown \cref{fig:rec_d2_correct_d} we get three \levelGroups at the end of recursion on $T_0(1)$.}
     	\label{fig:rec_d2_correct}
     \end{figure}
Level construction is performed on the \subgraph (\cref{fig:rec_d2_correct_c}) but the new levels only contain vertices of  $T_0(1)$, i.e. $L_1(1) = \{7^{3,1}\}$. Finally, \DTWO coloring by aggregation of two adjacent levels is performed, leading to three \levelGroups of stage 2 (\cref{fig:rec_d2_correct_d}), i.e. $T_1(0)=\{L_1(0) \cup L_1(1)\}$.  Now the vertices $3$ and $6$ are mapped to \levelGroups of different colors. Note, that the permutation step on the newly generated levels is not shown but performed as well to maintain data locality. 
    
%We finally apply the recursive step to the coloring shown in \cref{fig:2d_7pt_lb} where parallelism is restricted to five threads at a time.  To increase parallelism to eight threads we do apply our recursive approach 
%Requesting parallelism for eight threads for our \stex with \DTWO independency instead of five threads (see result of our procedure applying one recursive step to \levelGroups $T_0(4),T_0(5),T_0(6)$ and $T_0(7)$ is seen in \cref{fig:rec_2d-7pt_graph} (left). 
%In \cref{fig:rec_2d-7pt_graph} we demonstrate the impact of one recursive step applied to our \stex with \DTWO 
       \begin{figure}[H]
       	\begin{minipage}[c]{0.6\textwidth}
       		\includegraphics[height=0.3\textheight,width=0.89\textwidth]{pics/recursion/2d-7pt_example/2d-7pt/stencil_2d_7pt}
       	\end{minipage}\hfill
       	\begin{minipage}[c]{0.4\textwidth}
       	%	{\tt for parallel all \textcolor{red}{red}\\
       	%		\hspace*{1em} for parallel all \textcolor{amber}{orange}\\
       	%		\hspace*{1em} for parallel all \textcolor{magenta}{pink}\\
       	%	}
       	%	{\tt for parallel all \textcolor{blue}{blue}\\
       	%		\hspace*{1em} for parallel all \textcolor{carmine}{brown}\\
       	%		\hspace*{1em} for parallel all \textcolor{cyan}{cyan}\\
       	%	}
	       	\begin{tabular}{l|l}
	    %   		\toprule
	       		{Initial Stage} & {Recursion}\\
	       		{($s=0$)} & {($s=1$)}\\
	       		\midrule
	       	   \multirow{2}{*}{\textcolor{red}{red}} & {\textcolor{amber}{orange}}\\
	       	   \cmidrule(lr){2-2}
	       		& {\textcolor{magenta}{pink}}\\
	       		\midrule
	       	   \multirow{2}{*}{\textcolor{blue}{blue}} & {\textcolor{carmine}{brown}}\\
	       	   \cmidrule(lr){2-2}
	       	   & {\textcolor{cyan}{cyan}}\\
	       	   \bottomrule
	       	\end{tabular}
       		\caption{Graph coloring of the \stex for eight threads. Recursion is applied on \levelGroups $T_0(4-7)$ with two threads assigned to each. The parallel execution order is shown at top right.  Horizontal lines indicate synchronisation and its extent. Vertical lines distinguishes between \levelGroups of different stage (here $T_0$ and $T_1$) which can run in parallel. 
		%{\GW do i need to synchronize all threads working on orange color? or only those within the original \levelGroup, e.g. $T_0(4)$ ?} } {\CA only those within original \levelGroup $T_0(4)$, that is the reason why I used the term nested parallelism
		}
       		\label{fig:rec_2d-7pt_graph}
       	\end{minipage}
       \end{figure}
     
%Here we see recursion is applied to \levelGroups $T_0(4),T_0(5),T_0(6)$ and $T_0(7)$. In this case each of the \levelGroups where recursion is applied spawns parallelism for two threads. The selection of \levelGroups to refine and number of threads needed from each recursion are determined using a global load balancing technique as will be explained in \cref{subsec:subgraph_selection}.
%\Cref{fig:rec_2d-7pt_graph} (right) shows the execution order of different \levelGroups. Note the usage of nested parallelism, \ie for example thread responsible for $T_0(4)$ spawns two child threads to execute $T_1(0) \subset T_0(4)$ and $T_1(2) \subset T_0(4)$ in one parallel sweep, and $T_1(1) \subset T_0(4)$ and $T_1(3) \subset T_0(4)$ in next parallel sweep. At the end of each sweep there is synchronization between threads assigned to \levelGroup of similar color as represented by a horizontal bar in \cref{fig:rec_2d-7pt_graph} (right). Since each of the leaves needs to synchronize only with it's siblings (leaves of same parent)  we use a simple point to point synchronization scheme. 
          
\subsubsection{Level group construction and global load balancing} \label{subsec:subgraph_selection}

The recursive refinement of \levelGroups allows us to tackle load imbalance problems and limited degree of parallelism as we are no longer restricted by the one thread per \levelGroup constraint.  Instead we have the opportunity to form \levelGroups and assign appropriate thread counts to them such that the load per thread approaches the optimal value, i.e. total workload divided by the number of threads available. For simplicity we enforce equal number of threads to adjacent \levelGroups having different color, i.e. $T_s(i)$ and $T_s(i+1)$ with $i=0,2,4,...$. We then apply recursion to the \levelGroups with more than one thread assigned. Starting with the original graph as base \levelGroup ($T_{-1}(0)$) to which all available threads $n_t(T_{-1}(0))=\nthreadsMath$ and all vertices $N_r(T_{-1}(0))=\nrowsMath^{total}$ are assigned to, we perform the following steps to form \levelGroups $T_s(:)$ at stage $s \ge 0$  to which we assign $n_t(T_{s}(:))$ threads. To show the procedure we use the $16 \times 16$ \stex and construct a coloring scheme for eight threads (see \cref{fig:rec_2d-7pt_graph}).

\begin{enumerate}
	\item  Assign weights to all levels at stage ($s$) of the recursion. Assuming that $L_s(i) \subset T_{s-1}(j)$ its weight is defined by
	
	\begin{align*}
		w(L_s(i)) &= \frac{N_r(L_s(i))}{\frac{N_r(T_{s-1}(j))}{n_t(T_{s-1}(j))}} = \frac{N_r(L_s(i))}{N_r(T_{s-1}(j))}  n_t(T_{s-1}(j))\\
%		\nthreadsMath &: \text{total number of threads to be used for computing $T_{s-1}(j)$}\\
%		\nrowsMath^{total} &= n_r(T_{s-1}(j)) %\text{, i.e. number of vertices in graph}
	\end{align*}
	%In the initial stage ($s=0$) the base \levelGroup ($T_{-1}(0)$) is the full matrix/graph and $\nthreadsMath$ is the total number of threads available. 
	%This weight describes the fraction of the optimal load per thread ($\frac{n_r(T_{s-1}(j))}{n_t(T_{s-1}(j))}$) in the specific level ($L_s(i)$) for a given \levelGroup ($T_{s-1}(j)$) which has to be split up, i.e.  $n_t(T_{s-1}(j)) > 1$
	%{\CA May be rewrite sentence before as:}
	
	 For a given \levelGroup ($T_{s-1}(j)$) that has to be split up ( $n_t(T_{s-1}(j)) > 1$), the weight describes the fraction of the optimal load per thread  ($\frac{N_r(T_{s-1}(j))}{n_t(T_{s-1}(j))}$) in the specific level ($L_s(i)$).

	Requesting $n_t(T_{-1}(0))=8$ threads for the $N_r(T_{-1}(0)) = 256$ vertices of the \stex in \cref{fig:rec_2d-7pt_graph} produces these weights for the initial ($s=0$) levels
	\begin{align*}
		\{w(L_0(0)), w(L_0(1)), w(L_0(2)), ...\} &= \{\frac{1}{256} \times 8 , \frac{2}{256} \times 8 , \frac{3}{256} \times 8 , ...\}
	\end{align*}
	
	\item The above definition implies that if the weight is close to a natural number $b$, the corresponding workload is close to optimal for operation with $b$ threads. Thus, starting with $L_s(0)$ we aggregate successive levels until their combined weight forms a number $a$ close to a natural number $b$. Distance-k coloring is ensured by enforcing to aggregate \atleast $2 \times k$ \levels, \ie for \DTWO coloring at least four levels (two for red and two for blue) are included. Closeness of the number is measured by a parameter $\epsilon$ defined as
	\begin{align*}
		\epsilon &=  1 - abs(a-b), \text{where } b= max(1,nint(a))
	\end{align*}
	and controlled by the criterion
	\begin{align*}
	\epsilon &> \epsilon_s, \text{where $\epsilon_s \in  (0,1)$ are user defined parameters.} 	
	\end{align*}		   
	The choice of this parameter may be different for every stage of recursion. 
	 Once we find a collection of successive levels satisfying this criterion, the natural number $b$ is fixed. We try increasing the number of levels further to test if there exists a number $a'>a$ which is closer to $b$ leading to a $\epsilon$ value closer to one. We finally choose the set of levels with best $\epsilon$ value and define them to form $T_s(0)$ and $T_s(1)$ and to be executed by $n_t(T_s(0))=n_t(T_s(1))=b$ threads.	 
	 In \cref{fig:rec_2d-7pt_graph} we choose $\epsilon_s = 0.6$ which selects the first seven levels to form $T_0(0)$ and $T_0(1)$.  As their combined weight is $\frac{28}{32}=0.875$  one thread will execute these two \levelGroups. 
	  %$w(L_0(0)) + w(L_0(1)) + ... + w(L_0(5)) = 0.65625$ satisfies the criteria $\epsilon = 0.65625 > \epsilon_s$, and $b=1$. Therefore one thread ($b=1$) is to be assigned for $T_0(0)$ and  $T_0(1)$. Now fixing $b=1$ we check increasing levels and finally find $w(L_0(0)) + w(L_0(1)) + ... + w(L_0(6)) = 0.875$ gives the highest $\epsilon$ value. Thus for constructing weights of  first 7 levels are used.

	\item We continue with subsequent pairs of \levelGroups ($T_s(i), T_s(i+1); i=2,4 ...$) by applying this procedure starting with the very next \level. Finally, once all levels have been touched, a total $n_t(T_{s-1}(j))$ threads has been assigned to the levels $L_s(i) \subset T_{s-1}(j)$. For $T_0(4)$ and $T_0(5)$  in \cref{fig:rec_2d-7pt_graph} two threads satisfy the criterion as the total weight of the four levels included is $\frac{54}{32}=1.69$.
	
	\item The distribution between adjacent red and blue \levelGroups which are assigned to the same thread(s) as well as the final global load balancing is performed basically using the scheme presented in \cref{subsec:LB}. 
	%Here the calculation of mean and variance considers the weights of the levels and \levelGroups {\GW or better: considers the number of threads assigned to each \levelGroup ?}{\CA I wouldn't use levels here, I think the sentence in your comment (blue) is  better $\rightarrow$ }
	Here the calculation of mean and variance considers the number of threads ($n_t(T_{s}(j))$) assigned to each \levelGroup . Ideally, after this step the load per thread in each \levelGroup should approach the optimal value given above.
	% a \levelGroup with $b$ threads will ideally have $b$ times more load (vertices) than a \levelGroup with weight one. 
	The same algorithm \cref{alg:LB} as presented in \cref{subsec:LB} can be used but with slight modification. The modifications has been shown in the beginning of \cref{alg:LB}. The worker array now has to be replaced by the number of threads assigned to each \levelGroup ($n_t(T_{s}(j))$). The algorithm then tries to minimize the variance of number of vertices per thread in \levelGroups.
\end{enumerate}
Once the \levelGroup of stage $s$ has been formed, the recursion and the above procedure is separately applied to all new \levelGroups with more than one thread assigned. This continues until every \levelGroup is assigned to one thread. The depth of the recursion is determined by the parameter $\epsilon_s$ and depends on matrix structure as well as degree of parallelism requested. 
%\sout{ or the maximum recursion level (defined by user) is reached.} {\GW Is that correct?}{\CA No, since if recursion stops prematurely, some \levelGroups will have more than one thread assigned, and hence if these are not splitted by next stage recursion it will cause threads to have load imbalance. Only possibility to control this is via $\epsilon_s$ which can be reduced if many stages are not wished.}

For our \stex in  \cref{fig:rec_2d-7pt_graph} the inner four \levelGroups of stage $s=0$ required one stage of recursion. This leads to 16 \levelGroups at stage $s=1$, as we require four new \levelGroups per recursion to schedule two threads. 
In terms of parallel computation, first the red vertices will be computed in parallel with the orange ones using four threads for both colors. Once the orange vertices are done each pair of threads assigned to $T_0(4)$ and $T_0(6)$ synchronize locally (i.e. within $T_0(4)$ and $T_0(6)$ separately). Then the pink vertices are computed followed by a global synchronisation of all threads. The scheme continues with the blue vertices and the brown/cyan ones which represent the two blue \levelGroups to which recursion has been applied (see table in \cref{fig:rec_2d-7pt_graph}).  

The recursive nature of our scheme can be best  described by a tree data structure, where every node represents one \levelGroup and  the maximum depth is equivalent to the maximum level of recursion. The data structure for the colored graph in \cref{fig:rec_2d-7pt_graph} and its thread assignments is shown in \cref{fig:rec_2d-7pt_tree}. The root node represents our baseline \levelGroup $T_{-1}(0)$ comprising all 256 vertices and all eight threads (having unique $id=0,\ldots,7$). The first level of child nodes gives the initial ($s=0$) distribution, with each node storing the information of a \levelGroup including its color. Threads are mapped consecutively to the \levelGroups. As can be seen the red $T_0(4)$ \levelGroup which consists of vertices $66,\ldots,90$ (omitting the superscript $0$ for $s=0$) is executed by threads with $id=2,3$.  Applying recursion to $T_0(4)$ this node spawns four new child nodes at stage $s =1$, i.e. \levelGroups $T_1(0,\ldots,3)$, to be executed by the two threads. Synchronisation only happens between threads having the same parent node after executing the same color. Note that actual computations are only performed on the leaf nodes of the final tree.
	 \begin{figure}[thbp]
		 \includegraphics[width=\textwidth, height=0.2\textheight]{pics/recursion/2d-7pt_example/tree/tree}
	 	\caption{The internal tree structure of RACE representing the \stex for domain size $16 \times 16$, and eight threads. The range $[\ldots]$ specified in each leaf represents the vertices belonging to each \levelGroup and the id refers to the thread id assigned to each \levelGroups assuming compact pinning . The last entry $\langle$\nrowsEff$\rangle$ gives the effective row number introduced in  \cref{Sec:param_study}. }
	 	\label{fig:rec_2d-7pt_tree}
	 \end{figure}

\begin{comment}

{\GW Needs to be removed finally from here on till chapter 7}
%
%
%
While we achieved reasonable load balancing for the $16 \times 16$ \stex on five threads in ~\cref{fig:2d_7pt_lb}, requesting parallelization for eight threads will lead to massive load imbalance due to the \DTWO coloring constraint: At least one thread operating on two inner successive levels has a load of 31 vertices, e.g. if it operates on levels 15 and 16. However, the optimal load per thread is 16 vertices in a single color restricting the parallel speed-up to four, i.e. the parallel efficiency to 50 \%. {\CA This example between 5 and 8 threads is for the necessity of recursion and does not clearly stress the importance of global load balancing. One idea to motivate the importance of global load balancing is by telling currently with recursion we have 2 ways of parallelism one within a stage by increasing level groups of a stage (horizontal parallelism) and second by recursion on a single level group to form level groups in next stage (vertical parallelism). Choosing the right combination of both these parallelism levels require a global load balancing scheme. Current load balancing scheme \cref{subsec:LB} considers only the horizontal parallelism, now we have to extend it to incorporate also the vertical element of parallelism introduced by recursion.}

Thus, we have to modify/replace  {\GW Christie: can you comment on that (???)} {\CA (It is modification of the existing load balancing step, by allowing more threads per level group)} our original load balancing scheme {\CA{\sout{which assumed a single thread per \levelGroup}}}. {\CA The modification allows multiple threads to be assigned to a single level group $T_{s}(i)$ at stage $s$, threads that are assigned to specific level groups can then be spawned in the subsequent stages ($s+1, s+2, ...$) of recursion. The basic idea here is to assign multiple threads to \levelGroups containing bigger levels, and for simplicity we enforce equal number of threads to adjacent \levelGroups having different color.   To determine the number of threads responsible for a \levelGroup $T_{s}(i)$ following steps are followed:} 

\begin{enumerate}
	\item  First we assign weights to all levels in a specific stage ($s$) of the recursion. Assuming that $L_s(i) \subset T_{s-1}(j)$ its weight is defined by
	
	\begin{align*}
		w(L_s(i)) &= \frac{n_r(L_s(i))}{\frac{\nrowsMath^{total}}{\nthreadsMath^{}}}=\frac{n_r(L_s(i))}{\nrowsMath^{total}}  {\nthreadsMath^{}}\\
		\nthreadsMath &: \text{total number of threads to be used for computing $T_{s-1}(j)$}\\
		\nrowsMath^{total} &= n_r(T_{s-1}(j)) %\text{, i.e. number of vertices in graph}
	\end{align*}
	In the initial stage ($s=0$) the base \levelGroup ($T_{-1}(0)$) is the full matrix/graph and $\nthreadsMath$ is the total number of threads available. This weight describes which fraction of the optimal load per thread ($\frac{\nrowsMath^{total}}{\nthreadsMath^{}}$)  is available in the specific level. 

	For the example shown in \cref{fig:rec_2d-7pt_graph} requiring 8 threads ($T_{-1}(0) = 8$), weights would be 
	\begin{align*}
		\{w(L_0(0)), w(L_0(1)), w(L_0(2)), ...\} &= \{0.03125, 0.0625, 0.09375, ...\}
	\end{align*}
	
	\item The above definition of weight implies if the weight is close to a natural number $b$, the load induced by rows included in the calculation of weights is close to optimal for operation with $b$ threads. Thus starting with $L_s(0)$ we sum the weights till it forms a number $a$ close to a natural number $b$. Distance-k coloring is ensured by enforcing \atleast $2 \times k$ \levels in the sum, \ie for \DTWO coloring at least four levels (two for red and two for blue) are included. Closeness of the number is measured by a parameter $\epsilon$ defined as:
	\begin{align*}
		\epsilon &=  1 - abs(a-b)\\
		\text{where } b&= max(1,round(a))
	\end{align*}
	Closeness is controlled by the criteria:
	\begin{align*}
		\epsilon &> \epsilon_s, \text{where $\epsilon_s$ is a control parameter in the range of (0,1)} 	\end{align*}	
	   
	 Once we find a sum satisfying the criteria weight $b$ is fixed. Then we assign first \levelGroup of each color ($T_s(0)$ and $T_s(1)$) to be operated by $b$ threads. With fixed $b$ we try increasing levels in the \levelGroup to check if we can find a better natural number $a'$ closer to $b$ which would increase $\epsilon$ value, if found the new set is considered for allocating weights for $T_s(0)$ and $T_s(1)$. 
	 
	 Thus in case of our example in \cref{fig:rec_2d-7pt_graph} with $\epsilon_s = 0.6$,  $w(L_0(0)) + w(L_0(1)) + ... + w(L_0(5)) = 0.65625$ satisfies the criteria $\epsilon = 0.65625 > \epsilon_s$, and $b=1$. Therefore one thread ($b=1$) is to be assigned for $T_0(0)$ and  $T_0(1)$. Now fixing $b=1$ we check increasing levels and finally find $w(L_0(0)) + w(L_0(1)) + ... + w(L_0(6)) = 0.875$ gives the highest $\epsilon$ value. Thus for constructing weights of $T_0(0)$ and $T_0(1)$ first 7 levels are used.
	
	\item Weights of subsequent \levelGroups ($T_s(2), T_s(3), ...$) are found by repeating the summing procedure starting with the very next \level. Finally once all the  levels are touched $n_t$ threads would be allocated between pairs (red and blue) of \levelGroups. 
	
	For example in case of \cref{fig:rec_2d-7pt_graph} in order to find number of threads to be assigned for $T_0(2)$ and $T_0(3)$ we start summing from eighth level onwards ($w(L_0(7))$) and find one thread ($b=1$) satisfy the above criteria. For $T_0(4)$ and $T_0(5)$  two threads satisfy the criteria.
\end{enumerate}

Once all the weights are determined using this procedure a weighted load balancing scheme is employed. The scheme remains similar to the one seen in \cref{subsec:LB} except while calculating mean and variance the weights are considered such that after load balancing ideally a \levelGroup with weight $w$ will have $w$ times more load (rows) than a \levelGroup with weight one. Same algorithm \cref{alg:LB} presented in \cref{subsec:LB} can be used except now the weights are the $b$ values instead of ones's. As explained previously in \cref{subsec:LB} this also takes into account load balancing between different colors (red and blue).

%{\GW Christie: Please describe your scheme step by step with an itemize list and use examples from ~\cref{fig:rec_2d-7pt_graph}}

    
	\subsubsection{TO BE SKIPPED Internal representation of recursively generated \boldmath{\levelGroups}} \label{subsec:level_tree}
	The recursive nature of our procedure allows to exploit more parallelism. However this introduces more complexity and one has to additionally respect the dependencies between stages and still observe the dependencies within one stage. The best idea is to have a data structure similar to the recursion, therefore we extent the \levelPtr data structure to a hierarchical tree data structure to store these informations. This data structure is called a \levelTree. The root of \levelTree contains information of entire domain, first child leaves of this root \ie leaves with depth 1 stores information about \levelGroups in first stage ($T_0(..)$), leaves with depth 2 stores information about \levelGroups in second stage ($T_1(...)$) and so on. 
	
 
 \Cref{fig:rec_2d-7pt_tree} shows a \levelTree corresponding to \stex  with 8 threads seen in \cref{fig:rec_2d-7pt_graph}. The leaves in the tree correspond to different \levelGroups and store various informations like the range of vertices or the nodes that belong to this \levelGroup, the effective number of rows (\nrowsEff) that describes the quality of the method (which we will see later in \cref{Sec:param_study}), and other informations like {\tt thread\_id} assigned to specific \levelGroups.  Threads are assigned to each \levelGroup depending on the pinning strategy used. For example in \emph{fill} type pinning strategy one would pin thread 0 to $T_0(0)$ and $T_0(1)$, thread 1 to $T_0(2)$ and $T_0(3)$, thread 2 to $T_1(0)  \subset T_0(4)$, $T_1(1) \subset T_0(4)$, $T_1(0)  \subset T_0(5)$ and $T_1(1) \subset T_0(5)$, and so on as seen in \cref{fig:rec_2d-7pt_tree}. 
 
 Note that the \levelTree has a data structure that represents the nested parallelism being used as seen in \cref{fig:rec_2d-7pt_graph} (right). Therefore threads are spawned based on this \levelTree allowing for easy implementation of point to point synchronizations.

\subsubsection{TO BE SKIPPED Sub-graph selection and global load balancing} \label{subsec:subgraph_selection_old}
Parallelism required for hardware underneath can be obtained either by expanding the \levelTree horizontally \ie increasing \levelGroups within a stage or by expanding \levelTree vertically with the help of recursion. But as we have seen before in \cref{subsec:DK} the horizontal parallelism is limited by total levels (\totalLvl) and after a certain extent this would lead to load balancing problems. Similarly excessive usage of recursion is also not a good idea since data locality worsens due to local permutations within \subgraph. Therefore it is vital to find a proper balance and choose proper configuration. Furthermore just doing load balancing within a single stage is not the best, for example if we had equally balanced within stage 0 in \cref{fig:rec_2d-7pt_tree}, we would receive no benefit from recursion. Therefore a global load balancing becomes inevitable.

\Inorder to select proper \subgraph and do global load balancing we employ a simple algorithm to find proper weights for each \levelGroup ($T_s(i)$) in a particular stage, then depending on this weights, denoted as $w(T_s(i))$, we do load balancing with weights in the particular stage $s$ (algorithm for this is similar to \cref{alg:LB}, except weightage is given to \levelGroups). Finally if $w(T_s(i)) > 1$ we use recursion to achieve $w(T_s(i))$ parallel work in the next stage $T_{s+1}(i)$. The basic structure of the algorithm employed to find weights for the first stage is as follows:
\begin{enumerate}
	\item Find weights, $w(L_0(i))$ for each level in the current stage ($s$) by
		\begin{align*}
			w(L_0(i)) &= (\levelPtr_0[i+1] - \levelPtr_0[i])*\frac{\nthreadsMath}{\nrowsMath^{total}}\\
			\nthreadsMath &: \text{total parallelism required by hardware}\\
			\nrowsMath^{total} &: \text{number of vertices in graph}
		\end{align*}
	
	\item Starting from $w(L_0(0))$ sum up weights till they form a number ($a$) close to whole number ($b$). The closeness can be controlled by an efficiency parameter for stage $s$, $\epsilon_s$ is defined as:
	\begin{equation} \label{eq:epsilon}
		\epsilon_s =  1 - abs(a-b);
	\end{equation}
	All the \levels that are involved in the sum belongs to \levelGroups  operated by first thread in the current stage. The obtained number $b$ is chosen as weight for these \levelGroups \ie $w(T_0(0))=w(T_0(1))=b$. A local search is then done by increasing \levels in this \levelGroups to see if there is a better choice ($a$ close to $b$) with weight $b$, finally \levelGroups are formed with the best choice.  The weight for next \levelGroups are found by resetting the sum counter to zero and repeating the  procedure with \levels just after the current \levelGroups.
\end{enumerate}
For other recursive stage ($s\geq1$) same procedure can be applied to find weights $w(T_s(i))$, except that now $\nthreadsMath$ and $\nrowsMath^{total}$ has to be substituted with the threads required and vertices in the \subgraph considered.

\end{comment}