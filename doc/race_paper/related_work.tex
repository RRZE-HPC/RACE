% SIAM Shared Information Template
% This is information that is shared between the main document and any
% supplement. If no supplement is required, then this information can
% be included directly in the main document.

% Coloring and iterative solvers
One of the earliest work on parallelizing kernels within iterative methods having loop-carried dependencies is the red-black Gauss-Seidel 
scheme~\cite{RBGS}. 
Later Kamath and Sameh introduced a two-block partitioning scheme for parallelizing Kaczmarz method on tridiagonal structures~\cite{Kamath}. 
A first general study on the convergence of these parallel methods were done early in 1980 by Elfving~\cite{Elfving1980}. Another line of research focuses on parallelizing dependent kernels while maintaining the same convergence behavior of sequential execution. One of the earliest known works in this category is the hyperplane method~\cite{saad} on FDM (Finite Difference Method) like matrices. Extensions to this approach can be seen in~\cite{cm-rcm} where a hybrid approach between \MCfull and hyperplane method is used. However the most general method which falls into this category is level-scheduling~\cite{saad}.  Efficient implementation of this method can be attributed to Park \etal with his work on triangular solvers~\cite{park_ls}. Most of the above mentioned method have been tested only for their applicability to parallelize \DONE dependent kernels and some of them are not capable to deal with dependencies like \DTWO. The research on parallelizing \DONE dependent kernels has been strongly accelerated after the introduction of HPCG benchmark~\cite{hpcg}. When it comes to \DTWO kernels popular methods seen in the literature are locking based methods, thread private local vectors~\cite{thread_private_symm_spmv,sparseX} for kernels like symmetric sparse matrix vector or with the usage of specially tailored sparse matrix data formats like compressed sparse blocks (CSB)~\cite{CSB} or recursive sparse blocks (\RSB)~\cite{RSB}.

% Improving sparse matrix-vector multiply on cache architectures
Sparse matrix-vector multiplication is an integral part of many scientific algorithms beside Krylov-supspace methods. It is a bandwidth-limited operation and on cache-based architectures the main factors that influence performance are spatial locality in accessing the matrix, and temporal locality in re-using the elements of the vector. On of the first studies to improve  temporal locality was done by Toledo~\cite{Toledo:1997:IMP:279511.279532} who performed an extensive study of Cuthill-McKee (CM) ordering techniques on 3D finite-element test matrices when used in combination with blocking into small dense blocks. Various authors~\cite{Buluc:2011:RMA:2058524.2059503,Williams:2009:OSM:1513001.1513318,doi:10.1177/1094342004041296} used techniques such as cache blocking for symmetric SpMV multiplication by splitting the matrix into several smaller $p \times q$ sparse submatrices and presented an analytic cache-aware model to determine the optimal block size. Their algorithms and software are included in the software OSKI package~\cite{1742-6596-16-1-071} which is a collection of low-level primitives of tuned sparse kernels for modern cache-based superscalar machines.
Xing et. al.~\cite{Liu:2013:ESM:2464996.2465013} used similar techniques to improve SIMD efficiency on Intel Knights Landing architecture
and compared the performance againt various other many-core architectures such as NVIDIA Tesla K20X. More recent work can be found e.g. in~\cite{Buluc:2011:RMA:2058524.2059503}

Also previous work on parallel sparse matrix-vector multiplication has focused on reducing communication volume in a distributed memory setting, often by using graph or hypergraph partitioning techniques~\cite{Catalyurek:1999} Yzelman and Bisseling \cite{doi:10.1137/080733243,Yzelman-thesis-2011} extended the  hypergraph partitioning techniques by an cache-oblivious method for sparse matrix–vector multiplication. They permuted the rows and columns of the input matrix using a recursive hypergraph-based sparse matrix partitioning scheme so that the resulting matrix induces cache-friendly behavior during sparse matrix–vector multiplication.


% Coloring and symmetric sparse matrix multiply 

The advent of processors having more parallelism and the need to consider more unstructured matrices have made graph-based approach an important tool for parallelizing such kernels. Multicoloring is one of the most popular approach used in this field~\cite{MC}, but is sometimes not efficient on modern cache-based processors. There have been researches going on to increase the efficiency of \MCfull and improving the heuristics, an overview of the methods can be found in~\cite{equitable_color,dist_k_def,COLPACK}. One of the most successful and effective method in this regard is the \ABMCfull~\cite{ABMC} proposed by Iwashita \etal in 2012. 
% Sparse Matrix-Matrix Products Executed Through Coloring
In many applications it is important to compute a coloring with few colors in near-linear time \cite{doi:10.1137/13093426X}. In parallel, the optimistic (speculative) coloring method by Gebremedhin and Manne \cite{gebremedhin2000scalable} is the preferred approach~\cite{Boman:2016}. In this paper we present a range of implementation issues for this vertex-based optimistic approach. We also propose a xxxx optimistic approach that has more parallelism and bandwith performance.



