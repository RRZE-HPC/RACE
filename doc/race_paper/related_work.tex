% SIAM Shared Information Template
% This is information that is shared between the main document and any
% supplement. If no supplement is required, then this information can
% be included directly in the main document.
One of the earliest work on parallelizing kernels having loop-carried dependencies is the red-black Gauss-Seidel scheme \cite{RBGS}. Later Kamath and Sameh introduced a two-block partitioning scheme for parallelizing Kaczmarz method on tridiagonal structures \cite{Kamath}. A general study on the convergence of these block methods were done by Elfving in 1980 \cite{Elfving1980}.

The advent of processors having more parallelism and the need to consider more unstructured matrices have made graph-based approach an important tool for parallelizing such kernels. Multicoloring is one of the most popular approach used in this field \cite{MC}, but is sometimes not efficient on modern cache-based processors. There have been several researches going on to increase the efficiency of \MCfull and improving the heuristics, an overview of the methods can be found in \cite{equitable_color}. One of the most successful method in this regard is the \ABMCfull \cite{ABMC} proposed by Iwashita \etal in 2012. 

Another line of research focuses on parallelizing dependent kernels while maintaining the same convergence behavior of sequential execution. One of the earliest known works in this category is the hyperplane method \cite{saad} on FDM (Finite Difference Method) like matrices. Extensions to this approach can be seen in \cite{cm-rcm} where a hybrid approach between \MCfull and hyperplane method is used. However the most general method which falls into this category is level-scheduling \cite{saad}.  Efficient implementation of this method can be attributed to Park \etal with his work on triangular solvers \cite{park_ls}.

Most of the above mentioned method have been tested only for their applicability to parallelize \DONE dependent kernels and some of them are not capable to deal with dependencies like \DTWO. The research on parallelizing \DONE dependent kernels has been strongly accelerated after the introduction of HPCG benchmark \cite{hpcg}. When it comes to \DTWO kernels popular methods seen in the literature are locking based methods, thread private local vectors \cite{thread_private_symm_spmv,sparseX} for kernels like symmetric sparse matrix vector or with the usage of specially tailored sparse matrix data formats like compressed sparse blocks (CSB) \cite{CSB} or recursive sparse blocks (\RSB) \cite{RSB}.