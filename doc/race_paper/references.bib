@article{Kamath,
author = {Kamath, Chandrika and Sameh, Ahmed},
year = {1989},
month = {02},
pages = {291-312},
title = {A projection method for solving nonsymmetric linear systems on multiprocessors},
volume = {9},
doi = {10.1016/0167-8191(89)90114-2},
booktitle = {Parallel Computing}
}

@Article{RBGS,
 author = {Evans, D. J.},
 title = {{Parallel S.O.R. Iterative Methods}},
 journal = {Parallel Comput.},
 issue_date = {August, 1984},
 volume = {1},
 number = {1},
 month = aug,
 year = {1984},
 issn = {0167-8191},
 pages = {3--18},
 numpages = {16},
 doi = {10.1016/S0167-8191(84)90380-6},
 acmid = {1746042},
 publisher = {Elsevier Science Publishers B. V.},
 address = {Amsterdam, The Netherlands, The Netherlands},
 keywords = {Explicit group S.O.R. methods, asynchronous MIMD computer system, performance analysis, red-black ordering},
} 

@Article{Elfving1980,
author="Elfving, Tommy",
title="Block-iterative methods for consistent and inconsistent linear equations",
journal="Numerische Mathematik",
year="1980",
month="Mar",
day="01",
volume="35",
number="1",
pages="1--12",
abstract="We shall in this paper consider the problem of computing a generalized solution of a given linear system of equations. The matrix will be partitioned by blocks of rows or blocks of columns. The generalized inverses of the blocks are then used as data to Jacobi- and SOR-types of iterative schemes. It is shown that the methods based on partitioning by rows converge towards the minimum norm solution of a consistent linear system. The column methods converge towards a least squares solution of a given system. For the case with two blocks explicit expressions for the optimal values of the iteration parameters are obtained. Finally an application is given to the linear system that arises from reconstruction of a two-dimensional object by its one-dimensional projections.",
issn="0945-3245",
doi="10.1007/BF01396365",
url="https://doi.org/10.1007/BF01396365"
}

@article{MC,
 author = {Jones, Mark T. and Plassmann, Paul E.},
 title = {Scalable Iterative Solution of Sparse Linear Systems},
 journal = {Parallel Comput.},
 issue_date = {May 1994},
 volume = {20},
 number = {5},
 month = may,
 year = {1994},
 issn = {0167-8191},
 pages = {753--773},
 numpages = {21},
 doi = {10.1016/0167-8191(94)90004-3},
 acmid = {180110},
 publisher = {Elsevier Science Publishers B. V.},
 address = {Amsterdam, The Netherlands, The Netherlands},
 keywords = {conjugate gradient methods, distributed-memory computers, graph coloring heuristics, incomplete Cholesky, parallel algorithms, sparse matrices},
} 

@ARTICLE{equitable_color,
author={H. Lu and M. Halappanavar and D. Chavarr√≠a-Miranda and A. H. Gebremedhin and A. Panyala and A. Kalyanaraman},
journal={IEEE Transactions on Parallel and Distributed Systems},
title={Algorithms for Balanced Graph Colorings with Applications in Parallel Computing},
year={2017},
volume={28},
number={5},
pages={1240-1256},
doi = {10.1109/TPDS.2016.2620142},
keywords={graph colouring;multiprocessing systems;parallel architectures;balanced coloring heuristics;balanced graph colorings;balancing efficacy;distance-1 coloring;manycore architectures;multicore architectures;parallel community detection;parallel computing;Bipartite graph;Color;Context;Image color analysis;Processor scheduling;Standards;Balanced coloring;Tilera manycore architecture;community detection;distance-1 coloring;graph algorithms;parallel graph coloring;partial distance-2 coloring},
doi={10.1109/TPDS.2016.2620142},
ISSN={1045-9219},
month={May},
}

@inproceedings{ABMC,
 author = {Iwashita, Takeshi and Nakashima, Hiroshi and Takahashi, Yasuhito},
 title = {Algebraic Block Multi-Color Ordering Method for Parallel Multi-Threaded Sparse Triangular Solver in ICCG Method},
 booktitle = {Proceedings of the 2012 IEEE 26th International Parallel and Distributed Processing Symposium},
 series = {IPDPS '12},
 year = {2012},
 isbn = {978-0-7695-4675-9},
 pages = {474--483},
 numpages = {10},
 doi = {10.1109/IPDPS.2012.51},
 acmid = {2358625},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
 keywords = {ICCG method, linear iterative solver, multi-thread, reordering, sparse triangular solver},
} 

@book{saad,
author = {Saad, Y.},
title = {Iterative Methods for Sparse Linear Systems},
publisher = {Society for Industrial and Applied Mathematics},
year = {2003},
doi = {10.1137/1.9780898718003},
address = {},
edition   = {Second},
URL = {https://epubs.siam.org/doi/abs/10.1137/1.9780898718003},
eprint = {https://epubs.siam.org/doi/pdf/10.1137/1.9780898718003}
}

@InProceedings{cm-rcm,
author="Nakajima, Kengo
and Okuda, Hiroshi",
editor="Zima, Hans P.
and Joe, Kazuki
and Sato, Mitsuhisa
and Seo, Yoshiki
and Shimasaki, Masaaki",
title="Parallel Iterative Solvers for Unstructured Grids Using an OpenMP/MPI Hybrid Programming Model for the GeoFEM Platform on SMP Cluster Architectures",
booktitle="High Performance Computing",
year="2002",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="437--448",
abstract="An efficient parallel iterative method for unstructured grids developed by the authors for SMP cluster architectures on the GeoFEM platform is presented. The method is based on a 3-level hybrid parallel programming model, including message passing for inter-SMP node communication, loop directives by OpenMP for intra-SMP node parallelization and vectorization for each processing element (PE). Simple 3D elastic linear problems with more than 8{\texttimes}108 DOF have been solved by 3x3 block ICCG(0) with additive Schwarz domain decomposition and PDJDS/CM-RCM reordering on 128 SMP nodes of Hitachi SR8000/MPP parallel computer, achieving performance of 335.2 GFLOPS. The PDJDS/CM-RCM reordering method provides excellent vector and parallel performance in SMP nodes.",
isbn="978-3-540-47847-8"
}


@Inproceedings{park_ls,
 author = {Park, Jongsoo and Smelyanskiy, Mikhail and Sundaram, Narayanan and Dubey, Pradeep},
 title = {Sparsifying Synchronization for High-Performance Shared-Memory Sparse Triangular Solver},
 booktitle = {Proceedings of the 29th International Conference on Supercomputing - Volume 8488},
 series = {ISC 2014},
 year = {2014},
 isbn = {978-3-319-07517-4},
 location = {Leipzig, Germany},
 pages = {124--140},
 numpages = {17},
 doi = {10.1007/978-3-319-07518-1_8},
 acmid = {2769893},
 publisher = {Springer-Verlag New York, Inc.},
 address = {New York, NY, USA},
} 



@Techreport{hpcg,
author = {Dongarra, Jack and Heroux, Michael},
year = {2013},
month = {June},
pages = {},
title = {Toward a New Metric for Ranking High Performance Computing Systems},
number = {SAND2013-4744},
institution={Sandia National Laboratories}
}

@article{CARP,
author = {Dan Gordon and Rachel Gordon},
title = {Component-Averaged Row Projections: A Robust, Block-Parallel Scheme for Sparse Linear Systems},
journal = {SIAM Journal on Scientific Computing},
volume = {27},
number = {3},
pages = {1092-1117},
year = {2005},
doi = {10.1137/040609458},
}

@inproceedings{CSB,
 author = {Bulu\c{c}, Aydin and Fineman, Jeremy T. and Frigo, Matteo and Gilbert, John R. and Leiserson, Charles E.},
 title = {Parallel Sparse Matrix-vector and Matrix-transpose-vector Multiplication Using Compressed Sparse Blocks},
 booktitle = {Proceedings of the Twenty-first Annual Symposium on Parallelism in Algorithms and Architectures},
 series = {SPAA '09},
 year = {2009},
 isbn = {978-1-60558-606-9},
 location = {Calgary, AB, Canada},
 pages = {233--244},
 numpages = {12},
 doi = {10.1145/1583991.1584053},
 acmid = {1584053},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {compressed sparse blocks, compressed sparse columns, compressed sparse rows, matrix transpose, matrix-vector multiplication, multithreaded algorithm, parallelism, span, sparse matrix, storage format, work},
} 

@article{RSB,
 author = {Martone, Michele},
 title = {Efficient Multithreaded Untransposed, Transposed or Symmetric Sparse Matrix-vector Multiplication with the Recursive Sparse Blocks Format},
 journal = {Parallel Comput.},
 issue_date = {July, 2014},
 volume = {40},
 number = {7},
 month = jul,
 year = {2014},
 issn = {0167-8191},
 pages = {251--270},
 numpages = {20},
 url = {http://dx.doi.org/10.1016/j.parco.2014.03.008},
 doi = {10.1016/j.parco.2014.03.008},
 acmid = {2643477},
 publisher = {Elsevier Science Publishers B. V.},
 address = {Amsterdam, The Netherlands, The Netherlands},
 keywords = {Cache blocking, Shared memory parallel, Sparse matrix assembly, Sparse matrix-vector multiply, Symmetric matrix-vector multiply, Transpose matrix-vector multiply},
} 

@INPROCEEDINGS{thread_private_symm_spmv,
author={T. Gkountouvas and V. Karakasis and K. Kourtis and G. Goumas and N. Koziris},
booktitle={2013 IEEE 27th International Symposium on Parallel and Distributed Processing},
title={Improving the Performance of the Symmetric Sparse Matrix-Vector Multiplication in Multicore},
year={2013},
volume={},
number={},
pages={273-283},
keywords={indexing;matrix multiplication;multiprocessing systems;performance evaluation;sparse matrices;vectors;CG iterative method;CSX-Sym;baseline CSR implementation;compressed CSX format;indexing scheme;input matrix compression;local data minimization;local vector reduction phase;memory traffic minimization;multicore architectures;multithreaded SpMxV version;nonzero element symmetry;nonzero indexing compression scheme;symmetric SpMxV kernel;symmetric sparse matrix-vector multiplication kernel;Finite element analysis;Indexing;Instruction sets;Kernel;Sparse matrices;Symmetric matrices;Vectors;SpMV;compression;multicore optimization;sparse matrix-vector multiplication;symmetric sparse matrices},
doi={10.1109/IPDPS.2013.43},
ISSN={1530-2075},
month={May},}

@article{FEAST,
  author    = {Eric Polizzi},
  title     = {A Density Matrix-based Algorithm for Solving Eigenvalue Problems},
  journal   = {CoRR},
  volume    = {abs/0901.2665},
  year      = {2009},
  archivePrefix = {arXiv},
  eprint    = {0901.2665},
  timestamp = {Wed, 07 Jun 2017 14:42:37 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-0901-2665},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{feast_mc,
 author = {Galgon, Martin and Kr\"{a}mer, Lukas and Thies, Jonas and Basermann, Achim and Lang, Bruno},
 title = {On the Parallel Iterative Solution of Linear Systems Arising in the FEAST Algorithm for Computing Inner Eigenvalues},
 journal = {Parallel Comput.},
 issue_date = {November 2015},
 volume = {49},
 number = {C},
 month = nov,
 year = {2015},
 issn = {0167-8191},
 pages = {153--163},
 numpages = {11},
 doi = {10.1016/j.parco.2015.06.005},
 acmid = {2843273},
 publisher = {Elsevier Science Publishers B. V.},
 address = {Amsterdam, The Netherlands, The Netherlands},
 keywords = {CARP-CG, FEAST, Graphene modeling,, Multi-coloring, Parallel inner eigenvalue computation, Sparse linear systems},
} 

@misc{ESSEX,
title={{Equipping Sparse Solvers for Exascale - ESSEX}},
howpublished={\url{https://blogs.fau.de/essex/activities}}
}


@misc{UOF,
title={{SuiteSparse Matrix Collection}},
howpublished={\url{https://sparse.tamu.edu/}}
}

@article{Moritz_sell,
author = {Moritz Kreutzer and Georg Hager and Gerhard Wellein and Holger Fehske and Alan R. Bishop},
title = {{A Unified Sparse Matrix Data Format for Efficient General Sparse Matrix-Vector Multiplication on Modern Processors with Wide SIMD Units}},
journal = {SIAM Journal on Scientific Computing},
volume = {36},
number = {5},
pages = {C401-C423},
year = {2014},
doi = {10.1137/130930352},
}

@article{Williams_roofline,
 author = {Williams, Samuel and Waterman, Andrew and Patterson, David},
 title = {{Roofline: An Insightful Visual Performance Model for Multicore Architectures}},
 journal = {Commun. ACM},
 issue_date = {April 2009},
 volume = {52},
 number = {4},
 month = apr,
 year = {2009},
 issn = {0001-0782},
 pages = {65--76},
 numpages = {12},
 doi = {10.1145/1498765.1498785},
 acmid = {1498785},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{LIKWID,
author = {Treibig, J. and Hager, G. and Wellein, G.},
booktitle = {{Proceedings of PSTI2010, the First International Workshop on Parallel Software Tools and Tool Infrastructures}},
title = {LIKWID: A lightweight performance-oriented tool suite for x86 multicore environments},
year = {2010},
address = {San Diego CA},
}

@InProceedings{LIKWID_validation,
author="R{\"o}hl, Thomas
and Eitzinger, Jan
and Hager, Georg
and Wellein, Gerhard",
title="Validation of Hardware Events for Successful Performance Pattern Identification in High Performance Computing",
booktitle="Tools for High Performance Computing 2015",
year="2016",
publisher="Springer International Publishing",
address="Cham",
pages="17--28",
abstract="Hardware performance monitoring (HPM) is a crucial ingredient of performance analysis tools. While there are interfaces like LIKWID, PAPI or the kernel interface perf{\_}event which provide HPM access with some additional features, many higher level tools combine event counts with results retrieved from other sources like function call traces to derive (semi-)automatic performance advice. However, although HPM is available for x86 systems since the early 90s, only a small subset of the HPM features is used in practice. Performance patterns provide a more comprehensive approach, enabling the identification of various performance-limiting effects. Patterns address issues like bandwidth saturation, load imbalance, non-local data access in ccNUMA systems, or false sharing of cache lines. This work defines HPM event sets that are best suited to identify a selection of performance patterns on the Intel Haswell processor. We validate the chosen event sets for accuracy in order to arrive at a reliable pattern detection mechanism and point out shortcomings that cannot be easily circumvented due to bugs or limitations in the hardware.",
isbn="978-3-319-39589-0"
}

@ARTICLE{BFS,
author={C. Y. Lee},
journal={IRE Transactions on Electronic Computers},
title={An Algorithm for Path Connections and Its Applications},
year={1961},
volume={EC-10},
number={3},
pages={346-365},
keywords={Application software;Auditory system;Optical diffraction;Pattern recognition;Physical optics;Telephony;Transportation;Utility programs;Wiring},
doi={10.1109/TEC.1961.5219222},
ISSN={0367-9950},
month={Sept},
}

@article{BFS_level_def,
 author = {D\'{\i}az, Josep and Petit, Jordi and Serna, Maria},
 title = {A Survey of Graph Layout Problems},
 journal = {ACM Comput. Surv.},
 issue_date = {September 2002},
 volume = {34},
 number = {3},
 month = sep,
 year = {2002},
 issn = {0360-0300},
 pages = {313--356},
 numpages = {44},
 doi = {10.1145/568522.568523},
 acmid = {568523},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Approximation algorithms, complexity, embedding, heuristics, layout, parameterized complexity, random graphs},
}

@Inbook{RCM,
author="Cuthill, Elizabeth",
editor="Rose, Donald J.
and Willoughby, Ralph A.",
title="Several Strategies for Reducing the Bandwidth of Matrices",
bookTitle="Sparse Matrices and their Applications: Proceedings of a Symposium on Sparse Matrices and Their Applications, held September 9--10, 1971, at the IBM Thomas J. Watson Research Center, Yorktown Heights, New York, and sponsored by the Office of Naval Research, the National Science Foundation, IBM World Trade Corporation, and the IBM Research Mathematical Sciences Department.",
year="1972",
publisher="Springer US",
address="Boston, MA",
pages="157--166",
abstract="Systems of linear equations involving sparse, symmetric, positive definite coefficient matrices arise in many application areas. Our immediate interest in the solution of such systems has arisen from our use of the finite element displacement method for analyzing a wide variety of structures, and the need for automating the data generation for such analyses. Figure 1 shows a typical structure which has been modelled for analysis.",
isbn="978-1-4615-8675-3",
doi="10.1007/978-1-4615-8675-3_14",
url="https://doi.org/10.1007/978-1-4615-8675-3_14"
}


@article{RCM_Sparse_computation,
author = {Oliker, L. and Li, X. and Husbands, P. and Biswas, R.},
title = {Effects of Ordering Strategies and Programming Paradigms on Sparse Matrix Computations},
journal = {SIAM Review},
volume = {44},
number = {3},
pages = {373-393},
year = {2002},
doi = {10.1137/S00361445003820},
}


@article{sparseX,
 author = {Elafrou, Athena and Karakasis, Vasileios and Gkountouvas, Theodoros and Kourtis, Kornilios and Goumas, Georgios and Koziris, Nectarios},
 title = {SparseX: A Library for High-Performance Sparse Matrix-Vector Multiplication on Multicore Platforms},
 journal = {ACM Trans. Math. Softw.},
 issue_date = {April 2018},
 volume = {44},
 number = {3},
 month = jan,
 year = {2018},
 issn = {0098-3500},
 pages = {26:1--26:32},
 articleno = {26},
 numpages = {32},
 doi = {10.1145/3134442},
 acmid = {3134442},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {CSX, HPC, SpMV, SpMV library, data compression, high-performance computing, multicore, scientific applications, sparse matrix-vector multiplication},
} 

@INPROCEEDINGS{Park_HPCG,
author={J. Park and M. Smelyanskiy and K. Vaidyanathan and A. Heinecke and D. D. Kalamkar and X. Liu and M. M. A. Patwary and Y. Lu and P. Dubey},
booktitle={SC14: International Conference for High Performance Computing, Networking, Storage and Analysis},
title={Efficient Shared-Memory Implementation of High-Performance Conjugate Gradient Benchmark and its Application to Unstructured Matrices},
year={2014},
volume={},
number={},
pages={945-955},
keywords={conjugate gradient methods;iterative methods;matrix algebra;message passing;optimisation;parallel processing;shared memory systems;3D grid;CG convergence rate;Gauss-Seidel smoother parallelization;HPC applications;HPCG;MPI parallelization;TFLOPS;Tianhe-2 system;Xeon Phi shared-memory implementation;algorithmic optimizations;architecture-aware optimizations;block multicolor reordering;communication overhead;communication pattern;data access locality;high performance conjugate gradient benchmark;next generation extreme-scale computing systems;parallelism;sparse linear solvers;unstructured matrices;Benchmark testing;Convergence;Equations;Parallel processing;Sparse matrices;Synchronization;Vectors},
doi={10.1109/SC.2014.82},
ISSN={2167-4329},
month={Nov},}

@article{COLPACK,
 author = {Gebremedhin, Assefaw H. and Nguyen, Duc and Patwary, Md. Mostofa Ali and Pothen, Alex},
 title = {ColPack: Software for Graph Coloring and Related Problems in Scientific Computing},
 journal = {ACM Trans. Math. Softw.},
 issue_date = {September 2013},
 volume = {40},
 number = {1},
 month = oct,
 year = {2013},
 issn = {0098-3500},
 pages = {1:1--1:31},
 articleno = {1},
 numpages = {31},
 doi = {10.1145/2513109.2513110},
 acmid = {2513110},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Automatic differentiation, combinatorial optimization, graph coloring, greedy coloring algorithms, nonlinear optimization, sparse derivative computation, vertex ordering techniques},
} 

@inproceedings{dist_k_def,
 author = {Gebremedhin, Assefaw Hadish and Manne, Fredrik and Pothen, Alex},
 title = {Parallel Distance-k Coloring Algorithms for Numerical Optimization},
 booktitle = {Proceedings of the 8th International Euro-Par Conference on Parallel Processing},
 series = {Euro-Par '02},
 year = {2002},
 isbn = {3-540-44049-6},
 pages = {912--921},
 numpages = {10},
 url = {http://dl.acm.org/citation.cfm?id=646667.699892},
 acmid = {699892},
 publisher = {Springer-Verlag},
 address = {London, UK, UK},
} 

@article{METIS,
author = {George Karypis and Vipin Kumar},
title = {A Fast and High Quality Multilevel Scheme for Partitioning Irregular Graphs},
journal = {SIAM Journal on Scientific Computing},
volume = {20},
number = {1},
pages = {359-392},
year = {1998},
doi = {10.1137/S1064827595287997},
}

@misc{MKL,
  title =	 {Intel Math Kernel Library},
  author =	 {{Intel}},
  url =		 {https://software.intel.com/en-us/mkl},
  urldate =	 {2018/05/22},
}

@misc{SpMP,
  title =	 {Sparse matrix pre-processing library},
  author =	 {{SpMP Development Team}},
  url =		 {https://github.com/IntelLabs/SpMP},
  urldate =	 {2018/05/22},
}

@Misc{siam,
  key = {zzz},
  title =	 {{SIAM} Style Manual: For journals and books},
  year =	 2013,
url = {https://www.siam.org/journals/pdf/stylemanual.pdf}}

@Misc{Hi14,
  author =	 {Nick Higham},
  title =	 {A Call for Better Indexes},
  howpublished = {SIAM Blogs},
  year =	 2014,
  month =	 nov,
  url =		 {http://blogs.siam.org/a-call-for-better-indexes/},
  urldate =	 {2015-04-05}
}

@Misc{PeKoPi14,
  title = {Accelerating Community Detection by Using {K}-core Subgraphs},
  author =	 {Chengbin Peng and Tamara G. Kolda and Ali Pinar},
  eprint =	 {1403.2226},
  year =	 2014,
  month =	 mar,
  eprintclass =	 {math.NA}
}


@Article{WoZhMeSh05,
  author =	 {Woessner, Donald E. and Zhang, Shanrong and
                  Merritt, Matthew E. and Sherry, A. Dean},
  title = {Numerical Solution of the {Bloch} Equations Provides Insights 
                  into the Optimum Design of {PARACEST} Agents for {MRI}},
  journal =	 {Magnetic Resonance in Medicine},
  doi =		 {10.1002/mrm.20408},
  volume =	 53,
  number =	 4,
  month =	 apr,
  year =	 2005,
  pages =	 {790--799},
  archiveprefix = {PubMed},
  archive =	 {https://www.ncbi.nlm.nih.gov/pubmed},
  eprint =	 {15799055}
}

@Article{Ne03,
  title =	 {Properties of Highly Clustered Networks},
  author =	 {Newman, M. E. J.},
  doi =		 {10.1103/PhysRevE.68.026121},
  journal =	 {Phys. Rev. E},
  volume =	 {68},
  year =	 {2003},
  eid =		 {026121},
  pagetotal =	 6,
  month =	 aug,
}



@misc{clawpack,
  title =	 {Clawpack Software},
  author =	 {{Clawpack Development Team}},
  url =		 {http://www.clawpack.org},
  urldate =	 {2015/05/14},
  note =	 {Version 5.2.2},
  year =	 2015
}

@Misc{AMSMSC2010,
  title =	 {{Mathematics Subject Classification}},
  author =	 {{American Mathematical Society}},
  year =	 {2010},
  url =		 {http://www.ams.org/mathscinet/msc/msc2010.html},
  urldate =	 {2015/03/29},
}


@book{La86,
  author =	 "Leslie Lamport",
  title =	 "\LaTeX: A Document Preparation System",
  publisher =	 "Addison--Wesley",
  year =	 "1986",
  address =	 "Reading, MA"
}

@Book{MiGo04,
  author =	 {Frank Mittlebach and Michel Goossens},
  title =	 {The \LaTeX\ Companion},
  publisher =	 {Addison--Wesley},
  year =	 2004,
  edition =	 {2nd}}

@Book{GoVa13,
  author =	 {Golub, Gene H. and Van Loan, Charles F.},
  title =	 {Matrix Computations},
  publisher =	 {The Johns Hopkins University Press},
  address =	 {Baltimore},
  year =	 2013,
  edition =	 {4th}
}


@Misc{CalcI,
  author =	 {Paul Dawkins},
  title =	 {Paul's Online Math Notes: Calculus {I} --- Notes},
  url =               {http://tutorial.math.lamar.edu/Classes/CalcI/MeanValueTheorem.aspx},
  urldate =	 {2015-07-08}}

@Misc{amsmath,
  author =	 {{American Mathematical Society}},
  title =	 {User's Guide for the \texttt{amsmath} Package
                  (Version 2.0)},
  url =		 {ftp://ftp.ams.org/pub/tex/doc/amsmath/amsldoc.pdf},
  urldate =	 {2015-07-30},
  year =	 2002}

% Olaf addes July 16, 2018


@inproceedings{Liu:2013:ESM:2464996.2465013,
 author = {Liu, Xing and Smelyanskiy, Mikhail and Chow, Edmond and Dubey, Pradeep},
 title = {Efficient Sparse Matrix-vector Multiplication on x86-based Many-core Processors},
 booktitle = {Proceedings of the 27th International ACM Conference on International Conference on Supercomputing},
 series = {ICS '13},
 year = {2013},
 isbn = {978-1-4503-2130-3},
 location = {Eugene, Oregon, USA},
 pages = {273--282},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/2464996.2465013},
 doi = {10.1145/2464996.2465013},
 acmid = {2465013},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {esb format, intel many integrated core architecture (intel mic), intel xeon phi, knights corner, spmv},
 abstract = {Sparse matrix-vector multiplication (SpMV) is an important kernel in many scientific applications and is known to be memory bandwidth limited. On modern processors with wide SIMD and large numbers of cores, we identify and address several bottlenecks which may limit performance even before memory bandwidth: (a) low SIMD efficiency due to sparsity, (b) overhead due to irregular memory accesses, and (c) load-imbalance due to non-uniform matrix structures. We describe an efficient implementation of SpMV on the IntelR Xeon PhiTM Coprocessor, codenamed Knights Corner (KNC), that addresses the above challenges. Our implementation exploits the salient architectural features of KNC, such as large caches and hardware support for irregular memory accesses. By using a specialized data structure with careful load balancing, we attain performance on average close to 90% of KNC's achievable memory bandwidth on a diverse set of sparse matrices. Furthermore, we demonstrate that our implementation is 3.52x and 1.32x faster, respectively, than the best available implementations on dual IntelR XeonR Processor E5-2680 and the NVIDIA Tesla K20X architecture.}
} 


@inproceedings{Buluc:2011:RMA:2058524.2059503,
 author = {Buluc, Aydin and Williams, Samuel and Oliker, Leonid and Demmel, James},
 title = {Reduced-Bandwidth Multithreaded Algorithms for Sparse Matrix-Vector Multiplication},
 booktitle = {Proceedings of the 2011 IEEE International Parallel \& Distributed Processing Symposium},
 series = {IPDPS '11},
 year = {2011},
 isbn = {978-0-7695-4385-7},
 pages = {721--733},
 numpages = {13},
 url = {https://doi.org/10.1109/IPDPS.2011.73},
 doi = {10.1109/IPDPS.2011.73},
 acmid = {2059503},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
 abstract= {On multicore architectures, the ratio of peak memory bandwidth to peak floating-point performance (byte:flop ratio) is decreasing as core counts increase, further limiting the performance of bandwidth limited applications. Multiplying a sparse matrix (as well as its transpose in the unsymmetric case) with a dense vector is the core of sparse iterative methods. In this paper, we present a new multithreaded algorithm for the symmetric case which potentially cuts the bandwidth requirements in half while exposing lots of parallelism in practice. We also give a new data structure transformation, called bit masked register blocks, which promises significant reductions on bandwidth requirements by reducing the number of indexing elements without introducing additional fill-in zeros. Our work shows how to incorporate this transformation into existing parallel algorithms (both symmetric and unsymmetric) without limiting their parallel scalability. Experimental results indicate that the combined benefits of bit masked register blocks and the new symmetric algorithm can be as high as a factor of 3.5x in multicore performance over an already scalable parallel approach. We also provide a model that accurately predicts the performance of the new methods, showing that even larger performance gains are expected in future multicore systems as current trends (decreasing byte:flop ratio and larger sparse matrices) continue.}
} 

@article{doi:10.1137/13093426X,
author = {McCourt, M. and Smith, B. and Zhang, H.},
title = {Sparse Matrix-Matrix Products Executed Through Coloring},
journal = {SIAM Journal on Matrix Analysis and Applications},
volume = {36},
number = {1},
pages = {90-109},
year = {2015},
doi = {10.1137/13093426X},
URL = {https://doi.org/10.1137/13093426X},
eprint = {https://doi.org/10.1137/13093426X}
}

@article{doi:10.1177/1094342004041296,
author = {Eun-Jin Im and Katherine Yelick and Richard Vuduc},
title ={Sparsity: Optimization Framework for Sparse Matrix Kernels},
journal = {The International Journal of High Performance Computing Applications},
volume = {18},
number = {1},
pages = {135-158},
year = {2004},
doi = {10.1177/1094342004041296},
URL = {https://doi.org/10.1177/1094342004041296},
eprint = {https://doi.org/10.1177/1094342004041296},
abstract = { Sparse matrix‚Äìvector multiplication is an important computational kernel that performs poorly on most modern processors due to a low compute-to-memory ratio and irregular memory access patterns. Optimization is difficult because of the complexity of cache-based memory systems and because performance is highly dependent on the non-zero structure of the matrix. The SPARSITY system is designed to address these problems by allowing users to automatically build sparse matrix kernels that are tuned to their matrices and machines. SPARSITY combines traditional techniques such as loop transformations with data structure transformations and optimization heuristics that are specific to sparse matrices. It provides a novel framework for selecting optimization parameters, such as block size, using a combination of performance models and search.In this paper we discuss the optimization of two operations: a sparse matrix times a dense vector and a sparse matrix times a set of dense vectors. Our experience indicates that register level optimizations are effective for matrices arising in certain scientific simulations, in particular finite-element problems. Cache level optimizations are important when the vector used in multiplication is larger than the cache size, especially for matrices in which the non-zero structure is random. For applications involving multiple vectors, reorganizing the computation to perform the entire set of multiplications as a single operation produces significant speedups. We describe the different optimizations and parameter selection techniques and evaluate them on several machines using over 40 matrices taken from a broad set of application domains. Our results demonstrate speedups of up to 4√ó for the single vector case and up to 10√ó for the multiple vector case. }
}

@article{Catalyurek:1999,
author={U.V.~Catalyurek and C.~Aykanat},
journal={IEEE Trans. Parallel Distrib. Systems},
title={Hypergraph-partitioning-based decomposition for parallel sparse-matrix vector multiplication},
year={1999)},
volume={10},
number={7},
pages={673-693},
doi={10.1109/71.780863},
abstract = {In this work, we show that the standard graph-partitioning-based decomposition of sparse matrices does not reflect the actual communication volume requirement for parallel matrix-vector multiplication. We propose two computational hypergraph models which avoid this crucial deficiency of the graph model. The proposed models reduce the decomposition problem to the well-known hypergraph partitioning problem. The recently proposed successful multilevel framework is exploited to develop a multilevel hypergraph partitioning tool PaToH for the experimental verification of our proposed hypergraph models. Experimental results on a wide range of realistic sparse test matrices confirm the validity of the proposed hypergraph models. In the decomposition of the test matrices, the hypergraph models using PaToH and hMeTiS result in up to 63 percent less communication volume (30 to 38 percent less on the average) than the graph model using MeTiS, while PaToH is only 1.3-2.3 times slower than MeTiS on the average.}
}

@phdthesis{Yzelman-thesis-2011,
    title = "Fast sparse matrix‚Äìvector multiplication by partitioning and reordering",
    author = "Albert-Jan Nicholas Yzelman",
    school = "Utrecht University",
    address =" Utrecht",
    year = "2011"
}



@article{doi:10.1137/080733243,
author = {Yzelman, A. and Bisseling, R.},
title = {Cache-Oblivious Sparse Matrix‚ÄìVector Multiplication by Using Sparse Matrix Partitioning Methods},
journal = {SIAM Journal on Scientific Computing},
volume = {31},
number = {4},
pages = {3128-3154},
year = {2009},
doi = {10.1137/080733243},
URL = {https://doi.org/10.1137/080733243},
eprint = {https://doi.org/10.1137/080733243},
abstract = {In this article, we introduce a cache-oblivious method for sparse matrix‚Äìvector multiplication. Our method attempts to permute the rows and columns of the input matrix using a recursive hypergraph-based sparse matrix partitioning scheme so that the resulting matrix induces cache-friendly behavior during sparse matrix‚Äìvector multiplication. Matrices are assumed to be stored in row-major format, by means of the compressed row storage (CRS) or its variants incremental CRS and zig-zag CRS. The zig-zag CRS data structure is shown to fit well with the hypergraph metric used in partitioning sparse matrices for the purpose of parallel computation. The separated block-diagonal (SBD) form is shown to be the appropriate matrix structure for cache enhancement. We have implemented a run-time cache simulation library enabling us to analyze cache behavior for arbitrary matrices and arbitrary cache properties during matrix‚Äìvector multiplication within a k-way set-associative idealized cache model. The results of these simulations are then verified by actual experiments run on various cache architectures. In all these experiments, we use the Mondriaan sparse matrix partitioner in one-dimensional mode. The savings in computation time achieved by our matrix reorderings reach up to 50 percent, in the case of a large link matrix.}
}

@article{Toledo:1997:IMP:279511.279532,
 author = {Toledo, S.},
 title = {Improving the Memory-system Performance of Sparse-matrix Vector Multiplication},
 journal = {IBM J. Res. Dev.},
 issue_date = {Nov. 1997},
 volume = {41},
 number = {6},
 month = nov,
 year = {1997},
 issn = {0018-8646},
 pages = {711--726},
 numpages = {16},
 url = {http://dx.doi.org/10.1147/rd.416.0711},
 doi = {10.1147/rd.416.0711},
 acmid = {279532},
 publisher = {IBM Corp.},
 address = {Riverton, NJ, USA},
 note = {Toledo performs an extensive study of such ordering techniques, and found that CM ordering works well on 3D finite-element test matrices when used in combination with blocking into small dense blocks.}
}

@article{Williams:2009:OSM:1513001.1513318,
 author = {Williams, Samuel and Oliker, Leonid and Vuduc, Richard and Shalf, John and Yelick, Katherine and Demmel, James},
 title = {Optimization of Sparse Matrix-vector Multiplication on Emerging Multicore Platforms},
 journal = {Parallel Comput.},
 issue_date = {March, 2009},
 volume = {35},
 number = {3},
 month = mar,
 year = {2009},
 issn = {0167-8191},
 pages = {178--194},
 numpages = {17},
 url = {http://dx.doi.org/10.1016/j.parco.2008.12.006},
 doi = {10.1016/j.parco.2008.12.006},
 acmid = {1513318},
 publisher = {Elsevier Science Publishers B. V.},
 address = {Amsterdam, The Netherlands, The Netherlands},
 keywords = {Autotuning, Cell, HPC, Multicore, Niagara, Performance, Sparse},
 abstract = {We are witnessing a dramatic change in computer architecture due to the multicore paradigm shift, as every electronic device from cell phones to supercomputers confronts parallelism of unprecedented scale. To fully unleash the potential of these systems, the HPC community must develop multicore specific-optimization methodologies for important scientific computations. In this work, we examine sparse matrix-vector multiply (SpMV) - one of the most heavily used kernels in scientific computing - across a broad spectrum of multicore designs. Our experimental platform includes the homogeneous AMD quad-core, AMD dual-core, and Intel quad-core designs, the heterogeneous STI Cell, as well as one of the first scientific studies of the highly multithreaded Sun Victoria Falls (a Niagara2 SMP). We present several optimization strategies especially effective for the multicore environment, and demonstrate significant performance improvements compared to existing state-of-the-art serial and parallel SpMV implementations. Additionally, we present key insights into the architectural trade-offs of leading multicore design strategies, in the context of demanding memory-bound numerical algorithms.}
} 


  	
@article{1742-6596-16-1-071,
  author={Richard Vuduc and James W Demmel and Katherine A Yelick},
  title={OSKI: A library of automatically tuned sparse matrix kernels},
  journal={Journal of Physics: Conference Series},
  volume={16},
  number={1},
  pages={521},
  url={http://stacks.iop.org/1742-6596/16/i=1/a=071},
  year={2005},
  abstract={The Optimized Sparse Kernel Interface (OSKI) is a collection of low-level primitives that provide automatically tuned computational kernels on sparse matrices, for use by solver libraries and applications. These kernels include sparse matrix-vector multiply and sparse triangular solve, among others. The primary aim of this interface is to hide the complex decisionmaking process needed to tune the performance of a kernel implementation for a particular user's sparse matrix and machine, while also exposing the steps and potentially non-trivial costs of tuning at run-time. This paper provides an overview of OSKI, which is based on our research on automatically tuned sparse kernels for modern cache-based superscalar machines.}
}
	



@article{Boman:2016,
author={Mehmet~Deveci and Erik~G~Boman and Karen~D~Devine and Sivasankaran~Rajamanickam},
journal={IEEE International Parallel and Distributed Processing Symposium},
title={Parallel Graph Coloring for Manycore Architectures},
year={2016},
doi={10.1109/IPDPS.2016.54},
abstract = {Graph algorithms are challenging to parallelize on manycore architectures due to complex data dependencies and irregular memory access. We consider the well studied problem of coloring the vertices of a graph. In many applications it is important to compute a coloring with few colors in near-linear time. In parallel, the optimistic (speculative) coloring method by Gebremedhin and Manne is the preferred approach but it needs to be modified for manycore architectures. We discuss a range of implementation issues for this vertex-based optimistic approach. We also propose a novel edge-based optimistic approach that has more parallelism and is better suited to GPUs. We study the performance empirically on two architectures(Xeon Phi and GPU) and across many data sets (from finite element problems to social networks). Our implementation uses the Kokkos library, so it is portable across platforms. We show that on GPUs, we significantly reduce the number of colors (geometric mean 4X, but up to 48X) as compared to the widely used cuSPARSE library. In addition, our edge-based algorithm is 1.5 times faster on average than cuSPARSE, where it hasspeedups up to 139X on a circuit problem. We also show the effect of the coloring on a conjugate gradient solver using multi-colored Symmetric Gauss-Seidel method as preconditioner, the higher coloring quality found by the proposed methods reduces the overall solve time up to 33\% compared to cuSPARSE.}
}

@article{gebremedhin2000scalable,
  title={Scalable parallel graph coloring algorithms},
  author={Gebremedhin, Assefaw Hadish and Manne, Fredrik},
  journal={Concurrency: Practice and Experience},
  volume={12},
  number={12},
  pages={1131--1146},
  year={2000},
  publisher={Wiley Online Library}
}


@Misc{kokkos,
  title= {{Kokkos C++ Performance Portability Programming EcoSystem: The Programming Model - Parallel Execution and Memory Abstraction}},
  year = {2018}, 
  url = {http://trilinos.sandia.gov/packages/kokkos}
} 

@inproceedings{Liu:2015:CES:2751205.2751209,
 author = {Liu, Weifeng and Vinter, Brian},
 title = {CSR5: An Efficient Storage Format for Cross-Platform Sparse Matrix-Vector Multiplication},
 booktitle = {Proceedings of the 29th ACM on International Conference on Supercomputing},
 series = {ICS '15},
 year = {2015},
 isbn = {978-1-4503-3559-1},
 location = {Newport Beach, California, USA},
 pages = {339--350},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/2751205.2751209},
 doi = {10.1145/2751205.2751209},
 acmid = {2751209},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cpu, csr, csr5, gpu, sparse matrices, spmv, storage formats, xeon phi},
} 


@article{BOZDAG2008515,
title = "A framework for scalable greedy coloring on distributed-memory parallel computers",
journal = "Journal of Parallel and Distributed Computing",
volume = "68",
number = "4",
pages = "515 - 535",
year = "2008",
issn = "0743-7315",
doi = "https://doi.org/10.1016/j.jpdc.2007.08.002",
url = "http://www.sciencedirect.com/science/article/pii/S074373150700144X",
author = "Doruk Bozdaƒü and Assefaw H. Gebremedhin and Fredrik Manne and Erik G. Boman and Umit V. Catalyurek",
keywords = "Graph coloring, Parallel algorithms, Distributed-memory computers, Scientific computing, Experimental algorithmics"
}

@article{doi:10.1137/080732158,
author = {Bozdaƒü, D. and √áataly√ºrek, √ú. and Gebremedhin, A. and Manne, F. and Boman, E. and √ñzg√ºner, F.},
title = {Distributed-Memory Parallel Algorithms for Distance-2 Coloring and Related Problems in Derivative Computation},
journal = {SIAM Journal on Scientific Computing},
volume = {32},
number = {4},
pages = {2418-2446},
year = {2010},
doi = {10.1137/080732158},
URL = {https://doi.org/10.1137/080732158},
eprint = {https://doi.org/10.1137/080732158}
}
