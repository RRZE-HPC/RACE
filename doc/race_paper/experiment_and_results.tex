% SIAM Shared Information Template
% This is information that is shared between the main document and any
% supplement. If no supplement is required, then this information can
% be included directly in the main document.
The method stated above was implemented and consolidated into a library called \RACE. The library provides easy interface for parallelizing kernels, user typically just needs to supply the serial code (with dependency) and hardware settings. Library will then parallelize, pin and run the code in parallel. The library is publicly available in the git repository. %TODO

In the following we present the performance and convergence results obtained using the library, and compare it against state of art methods. Test setup, hardware and matrices as described in \cref{Sec:test_bed} is used for the following benchmarks.

\begin{figure}[thbp]
  	\centering
   	\subfloat[RACE performance compared to SpMV]{\label{fig:race_ivy}\includegraphics[width=0.49\textwidth, height=0.11\textheight]{pics/results/ivy/race}}
    \subfloat[SymmSpMV Comparison]{\label{fig:symm_spmv_ivy}\includegraphics[width=0.49\textwidth, height=0.11\textheight]{pics/results/ivy/symm_spmv}}
    \hspace{1em}
    \subfloat[GS Comparison]{\label{fig:gs_ivy}\includegraphics[width=0.49\textwidth, height=0.11\textheight]{pics/results/ivy/gs}}
    \subfloat[KACZ Comparison]{\label{fig:kacz_ivy}\includegraphics[width=0.49\textwidth, height=0.11\textheight]{pics/results/ivy/kacz}}
    \caption{Performance results on \IVB}
    \label{fig:ivy}
\end{figure}

\begin{figure}[thbp]
	\centering
	\subfloat[RACE performance compared to SpMV]{\label{fig:race_skx}\includegraphics[width=0.49\textwidth, height=0.11\textheight]{pics/results/skx/race}}
	\subfloat[SymmSpMV Comparison]{\label{fig:symm_spmv_skx}\includegraphics[width=0.49\textwidth, height=0.11\textheight]{pics/results/skx/symm_spmv}}
	\hspace{1em}
	\subfloat[GS Comparison]{\label{fig:gs_skx}\includegraphics[width=0.49\textwidth, height=0.11\textheight]{pics/results/skx/gs}}
	\subfloat[KACZ Comparison]{\label{fig:kacz_skx}\includegraphics[width=0.49\textwidth, height=0.11\textheight]{pics/results/skx/kacz}}
	\caption{Performance results on \SKX}
	\label{fig:skx}
\end{figure}

\subsection{Main points to discuss}
\begin{itemize}
	\item Relate roofline model and the performance graphs of RACE compared to SpMV. 
	\item Point out on \IVB we reach close to ideal performance in every case, and on \SKX except for corner cases like crankseg and offshore we reach close to ideal performance. The drop in corner cases like crankseg and offshore on \SKX is due to lack of parallelism attained by RACE and associated load imbalances. This effect shows up on \SKX rather than \IVB since \SKX has 20 threads compared to 10 on \IVB.
	\item Point out that for cases like Graphene, Spin, parabolic\_fem we don't see 2 fold increase in GFlop/s for KACZ, and SymmSpMV. This is due to the fact here \NNZR is very small like 4, 14 and 7 which causes two problems. For KACZ kernel there is one division per row and this causes a performance drop as evident in Spin matrices, also this effect can be observed for GS kernel. For SymmSpMV kernel the \NNZR decreases almost by half since we operate only on upper triangular part and with short loop over \NNZR no effective vectorization and modulo unrolling can be done.
	\item Matrices like crankseg-1, and offshore are also really small making some part of data fit in cache, this is the reason why they achieve performance above RLM.
	\item Discuss why we chose the methods for comparison. MC and ABMC are common in literature for \DONE coloring, MKL methods are standard library used in many productive codes, also it uses level-scheduling (not explicitly stated but we believe) for kernels like GS and enables us to compare with methods that do not disturb convergence. RSB enables to compare with methods using different data format and it has been shown this method has an upper hand in this category. 
	\item Comparison with SymmSpMV shows the behavior of different methods for \DTWO coloring. Here we see in almost all of the case RACE and RSB has an upper hand on \IVB. Although in some cases like offshore RACE clearly has an advantage. ABMC methods follow these methods. MKL and MC does not deliver good performance. For \SKX architecture RSB falls behind ABMC, we thing this is because of the requirement of ABMC to lock rows and cols of the submatrix on which a thread is working, becoming a bottleneck at high thread counts.
	\item Maybe tell RSB and 16-bit integer.
	\item Discuss with methods like ABMC and MC the performance especially drops for large matrices like Graphene, Spin, nlpkkt due to worsening of data locality ($\alpha$). Show sparsity pattern and \LIKWID meaurements. 
	\item Tell GS and KACZ performance includes also takes iterations into consideration (as shown in paper). Tell we do only a \DONE coloring for GS and \DTWO for KACZ. We use only matrices where GS can be applied and similarly for KACZ. Also we just compare against readily available solutions. Therefore RSB is left out for GS and RSB and MKL left out for KACZ.
	\item For GS RACE has an upper hand on \IVB and on \SKX RACE and ABMC have almost similar performance on \SKX, although for some cases RACE has huge advantage. Reason for this advantage is due to slight decrease in iterations for RACE (put fig) and slight improvement in performance compared to ABMC for \DONE case. For offshore case RACE performs worser that ABMC, this is because here with RACE one requires more iterations. Also note that all the large matrices which we had are unsuitable for GS sweep as they do not converge, but for large matrices the performance drops again for ABMC method due to degrading of $\alpha$ factor. (Maybe just put perf. pictures).
	\item Main advantage of RACE method comes with kernels having \DTWO dependencies like SymmSpMV and KACZ since here methods like ABMC require more colors and their locality degrades further since here within a color rows have to be structurally orthogonal (rows shouldn't have common column entries). Performance on KACZ shows this advantage. Here we again see for moderately large matrices the advantage is higher. Iteration behavior between methods remains similar to GS.
\end{itemize} 
     
