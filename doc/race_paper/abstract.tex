% SIAM Shared Information Template
% This is information that is shared between the main document and any
% supplement. If no supplement is required, then this information can
% be included directly in the main document.
Many iterative numerical methods for sparse systems and important building blocks of sparse linear algebra feature strong dependencies.These may be loop carried dependencies as they occur in many iterative solvers or preconditioners (e.g. of Gauss-Seidel (\GS) type) or write conflicts as they show up in parallelization of building blocks such as symmetric sparse matrix vector multiplication (\SymmSpmv) or sparse matrix transpose vector multiplication (\SpMTV). Scalable, hardware efficient parallelization of such kernels is known to be a hard problem in the general case of sparse matrices which do not have simple regular structures.

A standard approach to that problem is \MCfull (\MC) of the baseline matrix problems according to the requirements (e.g. distance-1 for GS type iterations) of the baseline kernel. For irregular and/or large matrices this basic methods may lead to load imbalances, frequent global synchronization and loss of data locality impacting single core and single node performance adversely. These problems typically increase for higher order distance colorings and larger matrices. Among several improvements strategies \ABMCfull (\ABMC) is the most prominent addressing above hardware problems. However, until now application of \ABMC has been shown only for distance-1 coloring problems. These approaches have in common that the baseline idea is not considering the requirements of modern hardware in terms of data locality, load balancing and degrees of parallelism available in modern compute nodes.

In this paper we present novel recursive algebraic coloring approach solving general \DK dependencies, which is motivated by the shortcomings of existing \MC methods in terms of hardware efficiency and parallelization overheads. Our method addresses matrices which have symmetric structure (but not necessarily symmetric matrix entries) and thus can be represented as an undirected graph. In a first step we do a \BFS pre-processing for bandwidth reduction of the graph, which aims to increase data locality for the underlying sparse matrix problems: Starting from a root vertex we construct the \levels of \BFS algorithm, i.e. \level $i$ consists all nodes having distance $i$ to the root vertex. We then permute the graph such that vertex numbering increases with distance from root vertex. Coloring the resulting \levels would be a naive approach to generate a \DK coloring but would for obvious reasons (e.g. \level 0 contains only one (root) vertex) often leading to severe load imbalances. Thus, we perform in a second step level aggregation of neighboring levels which aims at conserving data locality. The choice of the size of each \levelGroup is subject to two major constraints: First, for a \DK coloring of the original graph/problem at least $k$ \levels are aggregated into a \levelGroup (a.k.a supernode). This means that alternate \levelGroups can be executed in parallel which is equivalent to a \DONE coloring of the \levelGroups. Second, we apply a criterion for load balancing which considers the total amount of hardware threads to be used at execution time and tries to balance workload across these threads evenly. At this stage it might happen that most of the vertices may end up in a few \levelGroups. Therefore depending on the size of the level groups different number of threads will be assigned to them. \Inorder to further parallelize within this \levelGroup for assigned threads the entire procedure is recursively repeated on their corresponding \subgraph subject to \DK constraint. The aggregation step is controlled by a single external parameter which controls the load imbalance induced by forming each \levelGroup. Due to the recursive nature of this algorithm nested parallelism is required and one only needs local synchronization between the threads assigned to the same \subgraph.

We have implemented our algorithm in the open source library called \RACE (\RACEfull) which can either return all relevant data structures and parallelization information required for manual implementation of the kernel at hand. Or one could just use its callback function interface, where RACE takes care of parallelization and all data handling automatically. As of now RACE is limited to shared memory nodes as it only supports thread level parallelism. RACE uses CRS sparse matrix data format but can be easily extended to other formats.

Choosing a representative set of 28 sparse matrices we first perform an analysis of the impact of the aggregation parameter on the quality of the load balancing achieved for thread counts relevant for modern compute nodes. We observe that even for 60 threads 70\% of the test matrices achieve more than 70\% of effective parallelism (efficiency) after accounting for load imbalances.

Finally we apply RACE to two iterative schemes which require \DONE (\GS solver) and \DTWO (Kaczmarz solver) as well as for parallel symmetric sparse matrix vector multiplication (\SymmSpmv)  (where \DTWO coloring is required to resolve write conflicts). We analyze \RACE performance and compare with \MC using \COLPACK, \ABMC using \COLPACK on top of \METIS, Intel \MKL if the kernel is available and a data format which is tailored for \SymmSpmv. Overall we achieve a speed-up of $2-2.5 \times$ compared to \MC and \MKL implementations, while we are on par with \ABMC for small matrices and for large matrices we gain almost a factor of $1.5-2 \times$ benefit. Comparisons with iterative kernels also puts into light the convergence behavior of the method. Results show that the convergence of \RACE is better than \MC and is competitive with \ABMC method.