% SIAM Shared Information Template
% This is information that is shared \acrshort{ABMC} the main document and any
% supplement. If no supplement is required, then this information can
% be included directly in the main document.
The paper focuses on developing an alternative method to parallelize kernels having loop-carried dependencies. The method introduced here is applicable for solving general distance-k dependencies, similar to \acrfull{MC} methods. Currently we focus only on undirected graph \ie matrices with symmetric sparsity pattern (but not necessarily symmetric entries). The main motivation of the approach is to achieve good hardware performance on modern hardware architecture, by generating sufficient parallelism while preserving good data locality. The method needs no specialized data format, and works basically on simple sparse matrix format like \acrfull{CRS}.

Most of the above approaches explained above in \cref{Sec:related_work} suffer from performance penalties in one way or the other, for example \acrshort{MC} degrades the data locality, although this can be improved considerably using \acrfull{ABMC}, still for moderately large matrices or with the increase in $k$ of \DK dependency the method shows deterioration in performance. Similar drawbacks exists for other methods which will be discussed within this paper.

In this work we provide a detailed performance analysis of the method and comparison \acrshort{ABMC} different existing methods chosen from representative classes. The comparisons are done both for exact kernels like symmetric sparse matrix vector (\acrshort{SymmSpMV}) having \DTWO dependency and iterative solvers like \acrfull{KACZ}. For iterative scheme we further provide comparison \acrshort{ABMC} convergence of different methods. The comparisons are done on different hardware architectures ranging from Intel's \IVB series to modern \SKX architecture and the \AMD \EPY architecture. Here we also show the result of extension of \acrshort{ABMC} for \DTWO coloring which to our knowledge has not been studied previously. The comparisons shows the superiority of our method compared to others and the applicability of our method on wide-variety of heterogeneous systems. As far to our knowledge this is the first paper which demonstrates such high efficiency of \DTWO dependent kernels using simple and common \acrshort{CRS} matrix storage format on such broad scale of matrices.

The paper is limited to node level, and we use only thread level parallelization. Multi-node parallelization is left for future work. However it should be noted that for iterative kernels like \acrshort{KACZ} and \acrshort{GS} node-level performance is far more important because commonly such solvers are applied only locally and different approaches are used for parallelizing \acrshort{ABMC} nodes \cite{hpcg,CARP}.

\begin{comment}
As a final application run we demonstrate the parallelization of an eigen-value solver called FEAST \cite{FEAST}, where we use an iterative inner linear solver based on Kaczmarz method. The result presented is the first to achieve such high performance on node level for an iterative solver and is superior to the previous results published \cite{feast_mc}.
\end{comment}
