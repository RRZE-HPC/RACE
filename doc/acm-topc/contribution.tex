% SIAM Shared Information Template
% This is information that is shared \acrshort{ABMC} the main document and any
% supplement. If no supplement is required, then this information can
% be included directly in the main document.

\begin{itemize}
\item We present a recursive algebraic coloring scheme (RACE) which generates hardware efficient \DK colorings of undirected graphs. Special emphasis is put on achieving data locality and load balancing for shared memory parallelization.
\item We propose shared memory parallelization of \acrshort{SymmSpMV}  using a \DTWO coloring of the underlying undirected graph to avoid write conflicts and apply RACE for generating the colorings.
\item We provide a comprehensive performance study of shared memory parallel \acrshort{SymmSpMV} using RACE and compare with existing coloring methods as well as with vendor optimized library (Intel MKL), which may implement different parallelization approaches.
\item To validate the high quality of our implementation we further extend existing performance \acrshort{SpMV} modelling approaches to the \acrshort{SymmSpMV} kernel. Together with a single core performance analysis we also demonstrate that for specific classes of matrices  \acrshort{SymmSpMV}  single core as well as chip level performance may be negatively impacted by SIMD vectorization.
\item  Sustainability of our approach is demonstrated by using two very differenet generation of Intel Xeon processors (IVB and SKX)
\item --- Christie ---
	\item Problems with coloring approaches on SymmSpMV
	\item Performance modeling and analysis
	\item high alpha pre-factor - preserving data locality more important
	\item Things to take care on modern CPU's like SKX and caching
	\item Problems for large matrices and coloring
	\item RACE method - h/w efficient 
	\item General approach for D-k coloring
	\item Study on different corner cases like cache bound, low parallelism, memory bound, mixed effects 
	\item Why do we not necessarily get ideal 2 $\times$ speedup
	\item Problems for some special cases like low nnzr
	\item Comparisons with MKL and coloring approaches
\end{itemize}
We have implemented our graph coloring algorithms in the open source library \acrfull{RACE}\footnote{\href{https://bitbucket.org/christiealappatt/race}{https://bitbucket.org/christiealappatt/race \CAcomm{May be tinyurl is better???}}} and discuss the impact on a representative set of 31 sparse matrices. Most of the above coloring approaches explained above in \Cref{Sec:related_work} suffer from performance penalties in one way or the other; for example \acrshort{MC}, degrades the data locality, although this can be improved considerably using \acrshort{ABMC}, still for moderately large matrices or with the increase in $k$ of \DK dependency the method shows deterioration in performance. 
These coloring algorithms and related software frameworks do no take advantage of both load balancing as well as the existing deep memory hierarchy and the characteristics of today's compute nodes to achieve improved node-level performance at full scale.

Our experimental setup, the benchmark matrices, and the software framework are introduced in \Cref{Sec:test_bed}.
This paper presents the following contributions to the field of coloring and node-level performance engineering. In particular we introduce the computational intensity and main memory bandwidth for this kernel operation in \Cref{Sec:test_kernels} where we theoretically derive a realistic upper performance estimation for a given matrix nonzero pattern.  This performance estimation will be later used to evaluate scaling performance and data traffic on Intel's Ivy Bridge EP and Skylake SP. In \Cref{Sec:race} we focus on developing our hardware-aware recursive distance-k coloring that can be used as an alternative recursive method to parallelize kernels having loop-carried dependencies. By introducing an artificially designed stencil we will highlight the benefits of the method and how to construct levels, the usage of distance-k coloring, and how to obtain load balancing with up to 100 threads. Next we provide a detailed performance analysis of the method and comparison against \acrshort{MC} and \acrshort{ABMC}. In \Cref{Sec:param_study} we discuss the parameters selection and evaluate the performance of \acrshort{RACE} using the roofline performance model. In \Cref{Sec:expt} we compare the applicability of our recursive coloring method on Intel's Ivy Bridge and Skylake processor systems with more traditional approaches such as  \acrshort{MC}, \acrshort{ABMC}, and also the \acrshort{MKL} implementations. Finally we conclude in 
\Cref{Sec:conclusion}.


	
\begin{comment}
 \acrshort{RACE}  has two main usage scenarios: It can either return all relevant data structures and parallelization information required for manual implementation of the kernel at hand, or one can just use its callback function interface, which takes care of parallelization and all data handling automatically. As of now \acrshort{RACE} is limited to shared memory nodes as it only supports thread-level parallelism.\acrshort{RACE} uses the \acrfull{CRS} sparse matrix data format but can be easily extended to other formats.
\end{comment}


\begin{comment}
% Coloring and iterative solvers
Coloring techniques have also been extensively used within parallel
Krylov subspace methods. One of the earliest works on parallelizing
kernels within iterative methods having loop-carried dependencies is
the red-black Gauss--Seidel scheme~\cite{RBGS}. Later Kamath and Sameh
introduced a two-block partitioning scheme for parallelizing the
Kaczmarz method on tridiagonal structures~\cite{Kamath}. A first
general study on the convergence of these parallel methods was done
early in 1980 by Elfving~\cite{Elfving1980}. Another line of research
focuses on parallelizing dependent kernels while maintaining the same
convergence behavior of sequential execution. One of the earliest
known works in this category is the hyperplane method~\cite{saad} on
FDM (finite difference method) like matrices. Extensions to this
approach is given in ~\cite{cm-rcm} where a hybrid approach
between \acrshort{MC} and the hyperplane method is used. However, the
most general method which falls into this category is
level-scheduling{\CA should I remove hyphen?}~\cite{saad}.  Efficient
implementation of this method on triangular solvers can be attributed
to Park \etal ~\cite{park_ls}. Most of the above mentioned methods
have been tested only for their applicability to parallelize \DONE
dependent kernels and some of them are not capable of dealing with
dependencies like \DTWO. The research on parallelizing \DONE dependent
kernels has been strongly accelerated after the introduction of the
HPCG benchmark~\cite{hpcg}. When it comes to \DTWO kernels, popular
methods seen in the literature are locking-based methods, thread
private local vectors~\cite{sparseX,thread_private_symm_spmv} for
kernels like symmetric sparse matrix vector {\CA should it be
vectors?} or with the usage of specially tailored sparse matrix data
formats like compressed sparse blocks (CSB)~\cite{CSB}
or \acrfull{RSB}~\cite{RSB}.
\end{comment}



\begin{comment}
As a final application run we demonstrate the parallelization of an eigen-value solver called FEAST \cite{FEAST}, where we use an iterative inner linear solver based on Kaczmarz method. The result presented is the first to achieve such high performance on node level for an iterative solver and is superior to the previous results published \cite{feast_mc}.
\end{comment}
